# EfficientNet-B3 Food Classification Model Training

**Project**: NutriSight Food Recognition System  
**Model**: EfficientNet-B3 with Transfer Learning  
**Task**: Multi-class Image Classification (124 Food Categories)  
**Date**: October 2024  
**Framework**: PyTorch + DirectML (AMD/Intel GPU Support)

---

## ğŸ“Š Executive Summary

This document describes the training methodology and results for a deep learning model capable of recognizing **124 different food items** from images with **87.60% accuracy**.

### Key Results

- **Test Accuracy**: 87.60%
- **Top-5 Accuracy**: 96.96% (correct answer in top 5 predictions)
- **High-Confidence Predictions**: 98.38% accuracy (when model confidence â‰¥80%)
- **Training Time**: ~24 epochs (~27 hours on DirectML GPU)
- **Model Size**: 45MB (ONNX format for deployment)

---

## ğŸ¯ Problem Statement

**Goal**: Build an accurate food recognition system for nutritional tracking applications.

**Challenge**:

- Recognize 124 different food categories from photos
- Handle visual similarities between foods (e.g., different types of cakes, pasta dishes)
- Achieve high accuracy while maintaining reasonable inference speed
- Deploy on resource-constrained environments (web servers)

---

## ğŸ“ Dataset

### Dataset Structure

```
Total Images: 43,400
â”œâ”€â”€ Training Set: 34,720 images (80%)
â”œâ”€â”€ Validation Set: 4,340 images (10%)
â””â”€â”€ Test Set: 4,340 images (10%)

Food Categories: 124 classes
â”œâ”€â”€ Examples: Pizza, Hamburger, Sushi, Tacos, Ice Cream, etc.
â”œâ”€â”€ Images per class: ~350 images (balanced distribution)
â””â”€â”€ Image Resolution: 252Ã—252 pixels
```

### Data Source

- **Base Dataset**: Food-101 (101 food categories)
- **Extended Dataset**: Additional 23 food categories (Filipino cuisine focus)
- **Split Method**: Stratified random split to ensure balanced class distribution

---

## ğŸ§  Model Architecture

### Base Model: EfficientNet-B3

**Why EfficientNet-B3?**

1. **Efficient Design**: Balances accuracy and computational cost
2. **Compound Scaling**: Uniformly scales network depth, width, and resolution
3. **Pre-trained Weights**: Leverages ImageNet knowledge (transfer learning)
4. **Mobile-Friendly**: Suitable for deployment on resource-constrained devices

**Architecture Overview**:

```
Input Image (252Ã—252Ã—3)
    â†“
EfficientNet-B3 Backbone (12M parameters)
â”œâ”€â”€ Compound scaled CNN layers
â”œâ”€â”€ Mobile Inverted Bottleneck Convolution (MBConv)
â”œâ”€â”€ Squeeze-and-Excitation blocks
â””â”€â”€ Feature Extraction
    â†“
Custom Classification Head
â”œâ”€â”€ Dropout Layer (30% dropout rate)
â””â”€â”€ Fully Connected Layer (â†’ 124 classes)
    â†“
Output: Class Probabilities (124 values)
```

**Model Parameters**:

- Total Parameters: **12.0M**
- Trainable Parameters: **12.0M** (after warmup phase)
- Model Size: **~45MB** (ONNX format)

---

## ğŸ”¬ Training Methodology

### 1. Transfer Learning Strategy

**Two-Phase Training Approach**:

**Phase 1: Head Warmup (Epochs 1-3)**

- Freeze backbone (EfficientNet-B3 pre-trained layers)
- Train only classification head
- Purpose: Adapt final layers to food recognition task
- Learning Rate: 1Ã—10â»Â³

**Phase 2: Fine-tuning (Epochs 4-24)**

- Unfreeze entire network
- Train all layers with lower learning rate
- Purpose: Fine-tune feature extractors for food-specific patterns
- Learning Rate: 1Ã—10â»â´ (10Ã— reduction)

**Why This Approach?**

- Prevents catastrophic forgetting of pre-trained features
- Faster convergence compared to training from scratch
- Better generalization on limited dataset

---

### 2. Data Augmentation Techniques

**Purpose**: Artificially increase dataset diversity to improve generalization.

**Training Augmentations Applied**:

| Augmentation             | Parameters                                            | Purpose                              |
| ------------------------ | ----------------------------------------------------- | ------------------------------------ |
| **RandomResizedCrop**    | scale=(0.7, 1.0)                                      | Simulate different camera distances  |
| **RandomHorizontalFlip** | p=0.5                                                 | Handle left-right symmetry           |
| **RandomRotation**       | Â±20Â°                                                  | Account for camera tilt              |
| **ColorJitter**          | brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1 | Simulate lighting variations         |
| **RandomAffine**         | translate=(0.1, 0.1)                                  | Handle slight camera shifts          |
| **RandomErasing**        | p=0.2, scale=(0.02, 0.15)                             | Simulate occlusions (e.g., utensils) |

**Validation/Test Preprocessing**:

- Resize to 284Ã—284 pixels
- Center crop to 252Ã—252 pixels
- Normalize with ImageNet statistics

---

### 3. Regularization Techniques

**Purpose**: Prevent overfitting and improve model generalization.

#### A. Mixup Data Augmentation

- **Algorithm**: Blend two training images and their labels
- **Formula**:
  ```
  mixed_image = Î» Ã— image_A + (1-Î») Ã— image_B
  mixed_label = Î» Ã— label_A + (1-Î») Ã— label_B
  where Î» ~ Beta(0.2, 0.2)
  ```
- **Effect**: Forces model to learn more robust features
- **Parameter**: Î± = 0.2

#### B. Label Smoothing

- **Purpose**: Prevent overconfident predictions
- **Formula**:
  ```
  smoothed_label = (1 - Îµ) Ã— one_hot_label + Îµ/num_classes
  where Îµ = 0.1
  ```
- **Effect**: Improves calibration and generalization

#### C. Dropout

- **Rate**: 30% (p=0.3)
- **Location**: Before final classification layer
- **Effect**: Prevents co-adaptation of neurons

#### D. Weight Decay (L2 Regularization)

- **Rate**: 1Ã—10â»â´
- **Effect**: Penalizes large weights, promotes simpler models

---

### 4. Training Configuration

| Hyperparameter    | Value              | Rationale                               |
| ----------------- | ------------------ | --------------------------------------- |
| **Optimizer**     | AdamW              | Adaptive learning rates + weight decay  |
| **Learning Rate** | 1Ã—10â»Â³ â†’ 1Ã—10â»â´    | High warmup, lower fine-tuning          |
| **Batch Size**    | 16                 | Balance memory and gradient stability   |
| **Epochs**        | 50 (stopped at 24) | Early stopping prevented overfitting    |
| **Loss Function** | Cross-Entropy      | Standard for multi-class classification |
| **LR Scheduler**  | Cosine Annealing   | Smooth learning rate decay              |

---

### 5. Early Stopping

**Purpose**: Automatically stop training when model stops improving.

**Configuration**:

- **Patience**: 15 epochs
- **Metric**: Validation Top-1 Accuracy
- **Result**: Training stopped at epoch 24 (best: epoch 24)

**Why It Matters**:

- Prevents overfitting to training data
- Saves computational resources
- Ensures best model is used for deployment

---

## ğŸ“ˆ Training Results

### Learning Curves

#### Accuracy Over Epochs

```
Phase 1 (Head Warmup - Epochs 1-3):
â”œâ”€â”€ Val Accuracy: 53.02% â†’ 57.88% â†’ 59.79%
â””â”€â”€ Fast initial learning

Phase 2 (Full Fine-tuning - Epochs 4-24):
â”œâ”€â”€ Epoch 4: 77.51% (â†‘17.7% jump after unfreezing!)
â”œâ”€â”€ Epoch 7: 84.40%
â”œâ”€â”€ Epoch 10: 85.44%
â”œâ”€â”€ Epoch 15: 86.54%
â”œâ”€â”€ Epoch 21: 87.17%
â””â”€â”€ Epoch 24: 87.19% â† Best Model (stopped here)

Plateau Detection:
â””â”€â”€ Epochs 21-26: Fluctuating 86.5-87.2% (convergence)
```

#### Training Speed

```
Warmup Phase (Epochs 1-3):
â”œâ”€â”€ Speed: ~51 images/second
â””â”€â”€ Time per epoch: ~1,000 seconds (~17 minutes)

Fine-tuning Phase (Epochs 4-24):
â”œâ”€â”€ Speed: ~9.5 images/second
â””â”€â”€ Time per epoch: ~4,200 seconds (~70 minutes)

Reason for slowdown: Full model backpropagation (12M params)
```

---

## ğŸ¯ Final Model Performance

### Test Set Results (4,340 images)

| Metric                | Value  | Interpretation                       |
| --------------------- | ------ | ------------------------------------ |
| **Top-1 Accuracy**    | 87.60% | Correct on first guess 87.6% of time |
| **Top-5 Accuracy**    | 96.96% | Correct answer in top 5: 97% of time |
| **Precision (macro)** | 87.82% | Low false positive rate              |
| **Recall (macro)**    | 87.60% | Low false negative rate              |
| **F1 Score (macro)**  | 87.47% | Balanced precision/recall            |

### Confidence Analysis

**High-Confidence Predictions** (â‰¥80% confidence):

- **Percentage**: 34.12% of all predictions
- **Accuracy**: 98.38% (extremely reliable!)
- **Use Case**: Can trust these predictions without human review

**Medium-Confidence Predictions** (50-80% confidence):

- **Percentage**: ~42% of predictions
- **Accuracy**: ~85-90% (good but review recommended)

**Low-Confidence Predictions** (<50% confidence):

- **Percentage**: ~24% of predictions
- **Recommendation**: Flag for human review in production

---

## ğŸ“Š Per-Class Performance

### Best Performing Classes (100% accuracy)

Examples of foods the model recognizes perfectly:

- Apple: 100%
- Balut: 100%
- Frozen Yogurt: 100%
- Leche Flan: 100%
- Sunny Side Up: 100%
- Several Filipino dishes: Baked Tahong, Chicken Tinola, Daing na Bangus, Isaw Manok, Pritong Galunggong

**Why?** These foods have distinctive visual features, consistent appearance, and minimal intra-class variation.

### Challenging Classes (54-69% accuracy)

Foods with more variability or visual similarity:

- Chocolate Mousse: 54.3%
- Grilled Cheese Sandwich: 54.3%
- Omelette: 60.0%
- Pork Bistek: 60.0%
- Chocolate Cake: 62.9%
- Pork Chop: 65.7%
- Steak: 65.7%
- Tiramisu: 65.7%

**Why?** High visual similarity, regional variations, presentation differences, or overlapping ingredients with other classes.

---

## ğŸ”„ Comparison: Training vs Validation vs Test

| Metric         | Train   | Validation | Test   |
| -------------- | ------- | ---------- | ------ |
| Top-1 Accuracy | 99.74%  | 87.19%     | 87.60% |
| Top-5 Accuracy | 100.00% | 96.96%     | 96.96% |
| F1 Score       | 99.74%  | 87.13%     | 87.47% |

**Observations**:

- **Train >> Val/Test**: Expected behavior (model sees training data during learning)
- **Val â‰ˆ Test**: Excellent generalization (no overfitting!)
- **Gap (~12%)**: Reasonable for 124-class problem with regularization

---

## ğŸ§ª Technical Algorithms Used

### 1. **Convolutional Neural Networks (CNNs)**

- **Purpose**: Extract visual features from images
- **Components**: Convolution layers, pooling layers, activation functions
- **Why**: Effective for spatial pattern recognition in images

### 2. **Transfer Learning**

- **Concept**: Use knowledge from ImageNet (1000 classes) for food recognition
- **Benefit**: Reduces training time and improves accuracy with limited data

### 3. **Backpropagation with Adam Optimizer**

- **Purpose**: Update model weights to minimize loss
- **Algorithm**: Adaptive moment estimation (Adam)
- **Advantage**: Handles sparse gradients and noisy data well

### 4. **Cross-Entropy Loss Function**

- **Formula**: `Loss = -Î£(y_true Ã— log(y_pred))`
- **Purpose**: Measure difference between predicted and actual class
- **Why**: Standard for multi-class classification problems

### 5. **Softmax Activation**

- **Formula**: `softmax(x_i) = e^(x_i) / Î£(e^(x_j))`
- **Purpose**: Convert raw scores to probabilities (sum to 100%)
- **Output**: Confidence scores for each of 124 classes

### 6. **Cosine Annealing Learning Rate Scheduler**

- **Purpose**: Gradually reduce learning rate following cosine curve
- **Benefit**: Smooth convergence, avoids sharp changes in learning

---

## ğŸ’¾ Model Deployment

### Export Format: ONNX (Open Neural Network Exchange)

**Specifications**:

- **Input Shape**: `[batch_size, 3, 252, 252]`
- **Output Shape**: `[batch_size, 124]` (probability for each class)
- **File Size**: ~45MB
- **Runtime**: Compatible with ONNX Runtime (CPU/GPU)

**Inference Pipeline**:

```
1. Load image â†’ Resize to 252Ã—252
2. Normalize with ImageNet statistics
3. Run ONNX model inference
4. Apply softmax to get probabilities
5. Return top-5 predictions with confidence scores
```

**Performance**:

- **CPU Inference**: ~50-150ms per image
- **Memory Usage**: ~200-300MB
- **Deployment**: Web servers (Node.js/Express), Mobile apps, Edge devices

---

## ğŸ“ Key Learnings & Best Practices

### What Worked Well âœ…

1. **Two-phase training**: Warmup + fine-tuning prevented catastrophic forgetting
2. **Strong augmentation**: Mixup and geometric transforms improved generalization
3. **Early stopping**: Prevented overfitting and saved computation
4. **High-quality dataset**: Balanced, diverse images led to robust model

### Challenges Encountered âš ï¸

1. **Visually similar classes**: Salads, soups, and pasta dishes harder to distinguish
2. **Training time**: Full fine-tuning ~70 minutes/epoch on DirectML
3. **Class imbalance sensitivity**: Some rare foods need more training examples

### Future Improvements ğŸš€

1. **Ensemble models**: Combine multiple models for higher accuracy
2. **Data augmentation++**: Use advanced techniques (CutMix, AutoAugment)
3. **Larger models**: Try EfficientNet-B4 or B5 for marginal gains
4. **Active learning**: Collect more images for challenging classes

---

## ğŸ“š References & Resources

### Algorithms & Techniques

1. **EfficientNet**: Tan & Le (2019) - "EfficientNet: Rethinking Model Scaling for CNNs"
2. **Mixup**: Zhang et al. (2018) - "mixup: Beyond Empirical Risk Minimization"
3. **Label Smoothing**: Szegedy et al. (2016) - "Rethinking Inception Architecture"
4. **Transfer Learning**: Pan & Yang (2010) - "A Survey on Transfer Learning"

### Frameworks & Tools

- **PyTorch**: Deep learning framework
- **torchvision**: Pre-trained models and image transforms
- **torch-directml**: AMD/Intel GPU acceleration
- **scikit-learn**: Evaluation metrics
- **ONNX**: Model export for deployment

---

## ğŸ“‚ Project Files

```
nutrisight_model_training/
â”œâ”€â”€ train_efficientnet_b3_optimized.ipynb   # Training notebook
â”œâ”€â”€ README.md                                # This documentation
â””â”€â”€ efficientnet_b3_optimized-20251021-115920/
    â”œâ”€â”€ best_efficientnet_b3.pth            # Best model checkpoint (PyTorch)
    â”œâ”€â”€ model.onnx                          # Deployment model (ONNX)
    â”œâ”€â”€ class_names.json                    # 124 food category names
    â”œâ”€â”€ summary.json                        # Complete training statistics
    â”œâ”€â”€ metrics_epoch.csv                   # Per-epoch metrics
    â”œâ”€â”€ per_class_accuracy.json             # Accuracy for each food class
    â”œâ”€â”€ per_class_metrics.json              # Precision/Recall per class
    â”œâ”€â”€ classification_report.txt           # Detailed classification report
    â””â”€â”€ confusion_matrix.png                # Visual confusion matrix
```

---

## ğŸ¯ Conclusion

This study successfully developed a **high-accuracy food recognition model** using deep learning and transfer learning techniques. The final model achieves:

- âœ… **87.60% top-1 accuracy** on 124 food categories
- âœ… **96.96% top-5 accuracy** (almost always correct in top 5 guesses)
- âœ… **98.38% accuracy** on high-confidence predictions
- âœ… **Production-ready** ONNX model for web/mobile deployment
- âœ… **Robust performance** across validation and test sets (no overfitting)

The model is suitable for deployment in **nutritional tracking applications**, **restaurant menu digitization**, and **food recognition systems**.

### Statistical Significance

With 4,340 test images and 87.60% accuracy, the model correctly classifies **3,802 out of 4,340 images**, demonstrating strong real-world applicability for food recognition tasks.

---

**Model Training Date**: October 2024  
**Training Duration**: ~27 hours  
**Hardware**: DirectML-compatible GPU  
**Framework**: PyTorch 2.x + torchvision

---

_For questions or additional information, please refer to the training notebook: `train_efficientnet_b3_optimized.ipynb`_
