{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c9e5fd",
   "metadata": {},
   "source": [
    "# EfficientNet-B3 Food Classification - Optimized Training\n",
    "Train EfficientNet-B3 on splits_new_v2 dataset with:\n",
    "- Strong augmentation and regularization for maximum val accuracy\n",
    "- Comprehensive evaluation and metrics\n",
    "- ONNX export with class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008cf4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, time, json, csv, random, datetime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_directml\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "dml = torch_directml.device()\n",
    "print(\"Using device:\", dml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be446b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory: d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\n"
     ]
    }
   ],
   "source": [
    "# Paths and hyperparameters - optimized for max val accuracy\n",
    "root_dir = r\"d:\\VSC FILES\\testtrain\\splits_new_v2\"\n",
    "assert os.path.isdir(root_dir), f\"Dataset not found: {root_dir}\"\n",
    "\n",
    "model_name = \"efficientnet_b3\"\n",
    "use_pretrained = True\n",
    "\n",
    "# Optimized hyperparams\n",
    "image_size = 252  # native image size from dataset\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "num_workers = 0\n",
    "print_every = 50\n",
    "\n",
    "# Regularization\n",
    "label_smoothing = 0.1\n",
    "dropout_p = 0.3\n",
    "mixup_alpha = 0.2\n",
    "head_warmup_epochs = 3\n",
    "\n",
    "# Early stopping\n",
    "patience = 15  # stop if no improvement for 15 epochs\n",
    "\n",
    "run_root = os.path.join(os.path.dirname(root_dir), \"runs\")\n",
    "ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = os.path.join(run_root, f\"efficientnet_b3_optimized-{ts}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "print(f\"Run directory: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5308d99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 124 | train=34720 val=4340 test=4340\n",
      "Saved class names to d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\\class_names.json\n"
     ]
    }
   ],
   "source": [
    "# Dataset and DataLoaders with strong augmentation\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.15)),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize(image_size + 32),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "class SplitDataset(Dataset):\n",
    "    def __init__(self, root, split, transform=None, classes=None, class_to_idx=None):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        if classes is None or class_to_idx is None:\n",
    "            classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
    "            class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        samples = []\n",
    "        for cls in self.classes:\n",
    "            split_dir = os.path.join(root, cls, split)\n",
    "            if not os.path.isdir(split_dir):\n",
    "                continue\n",
    "            for dp, _, fns in os.walk(split_dir):\n",
    "                for fn in fns:\n",
    "                    if fn.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")):\n",
    "                        samples.append((os.path.join(dp, fn), self.class_to_idx[cls]))\n",
    "        if not samples:\n",
    "            raise RuntimeError(f\"No images for split='{split}' under {root}\")\n",
    "        self.samples = samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        with Image.open(path) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "train_ds = SplitDataset(root_dir, 'train', transform=train_tfms)\n",
    "val_ds = SplitDataset(root_dir, 'val', transform=eval_tfms, classes=train_ds.classes, class_to_idx=train_ds.class_to_idx)\n",
    "test_ds = SplitDataset(root_dir, 'test', transform=eval_tfms, classes=train_ds.classes, class_to_idx=train_ds.class_to_idx)\n",
    "num_classes = len(train_ds.classes)\n",
    "print(f\"Classes: {num_classes} | train={len(train_ds)} val={len(val_ds)} test={len(test_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "train_eval_ds = SplitDataset(root_dir, 'train', transform=eval_tfms, classes=train_ds.classes, class_to_idx=train_ds.class_to_idx)\n",
    "train_eval_loader = DataLoader(train_eval_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Save class names\n",
    "with open(os.path.join(run_dir, 'class_names.json'), 'w') as f:\n",
    "    json.dump(train_ds.classes, f, indent=2)\n",
    "print(f\"Saved class names to {os.path.join(run_dir, 'class_names.json')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38236ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 10.89M | Trainable: 0.19M\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "weights = models.EfficientNet_B3_Weights.DEFAULT if use_pretrained else None\n",
    "model = models.efficientnet_b3(weights=weights)\n",
    "in_f = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=dropout_p),\n",
    "    nn.Linear(in_f, num_classes)\n",
    ")\n",
    "model = model.to(dml)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "# Freeze backbone for warmup\n",
    "for name, p in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        p.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total params: {total_params/1e6:.2f}M | Trainable: {trainable_params/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "857114a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics and utilities\n",
    "def topk_accuracy(logits, targets, topk=(1, 5)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = logits.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append((correct_k.item() / targets.size(0)) * 100.0)\n",
    "    return res\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.sum = 0.0\n",
    "        self.n = 0\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / max(1, self.n)\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += float(val) * n\n",
    "        self.n += n\n",
    "\n",
    "def mixup_data(x, y, alpha=0.4):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, 1.0\n",
    "    lam = float(np.random.beta(alpha, alpha))\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "    mixed_x = lam * x + (1.0 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "log_csv = os.path.join(run_dir, \"metrics_epoch.csv\")\n",
    "log_jsonl = os.path.join(run_dir, \"metrics_epoch.jsonl\")\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e2384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    loss_m, top1_m, top5_m = AvgMeter(), AvgMeter(), AvgMeter()\n",
    "    t0 = time.perf_counter()\n",
    "    n_seen = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(dml)\n",
    "        labels = labels.to(dml)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if mixup_alpha > 0:\n",
    "            images, y_a, y_b, lam = mixup_data(images, labels, alpha=mixup_alpha)\n",
    "            y_a = y_a.to(dml)\n",
    "            y_b = y_b.to(dml)\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        bsz = labels.size(0)\n",
    "        n_seen += bsz\n",
    "        top1, top5 = topk_accuracy(outputs, labels, (1, 5))\n",
    "        loss_m.update(loss.item(), bsz)\n",
    "        top1_m.update(top1, bsz)\n",
    "        top5_m.update(top5, bsz)\n",
    "        \n",
    "        if (i + 1) % print_every == 0 or (i + 1) == len(train_loader):\n",
    "            dt = time.perf_counter() - t0\n",
    "            ips = n_seen / max(1e-6, dt)\n",
    "            print(f\"Epoch {epoch:02d} | batch {i+1}/{len(train_loader)} | loss {loss_m.avg:.4f} | top1 {top1_m.avg:.2f}% | top5 {top5_m.avg:.2f}% | {ips:.1f} img/s\")\n",
    "    return {\"loss\": loss_m.avg, \"top1\": top1_m.avg, \"top5\": top5_m.avg}\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    loss_m, top1_m, top5_m = AvgMeter(), AvgMeter(), AvgMeter()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(dml)\n",
    "            labels = labels.to(dml)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            top1, top5 = topk_accuracy(outputs, labels, (1, 5))\n",
    "            bsz = labels.size(0)\n",
    "            loss_m.update(loss.item(), bsz)\n",
    "            top1_m.update(top1, bsz)\n",
    "            top5_m.update(top5, bsz)\n",
    "    return {\"loss\": loss_m.avg, \"top1\": top1_m.avg, \"top5\": top5_m.avg}\n",
    "\n",
    "def log_epoch(row):\n",
    "    write_header = not os.path.exists(log_csv)\n",
    "    with open(log_csv, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
    "        if write_header:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "    with open(log_jsonl, \"a\") as f:\n",
    "        f.write(json.dumps(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39a7da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JP\\anaconda3\\envs\\dml\\Lib\\site-packages\\torch\\optim\\adamw.py:529: UserWarning: The operator 'aten::lerp.Scalar_out' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at C:\\__w\\1\\s\\pytorch-directml-plugin\\torch_directml\\csrc\\dml\\dml_cpu_fallback.cpp:17.)\n",
      "  torch._foreach_lerp_(device_exp_avgs, device_grads, 1 - beta1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | batch 50/2170 | loss 4.8932 | top1 2.00% | top5 6.75% | 46.4 img/s\n",
      "Epoch 01 | batch 100/2170 | loss 4.7614 | top1 3.56% | top5 10.38% | 48.9 img/s\n",
      "Epoch 01 | batch 100/2170 | loss 4.7614 | top1 3.56% | top5 10.38% | 48.9 img/s\n",
      "Epoch 01 | batch 150/2170 | loss 4.6581 | top1 4.88% | top5 13.88% | 50.3 img/s\n",
      "Epoch 01 | batch 150/2170 | loss 4.6581 | top1 4.88% | top5 13.88% | 50.3 img/s\n",
      "Epoch 01 | batch 200/2170 | loss 4.5659 | top1 6.03% | top5 15.50% | 50.5 img/s\n",
      "Epoch 01 | batch 200/2170 | loss 4.5659 | top1 6.03% | top5 15.50% | 50.5 img/s\n",
      "Epoch 01 | batch 250/2170 | loss 4.4787 | top1 6.83% | top5 16.90% | 50.7 img/s\n",
      "Epoch 01 | batch 250/2170 | loss 4.4787 | top1 6.83% | top5 16.90% | 50.7 img/s\n",
      "Epoch 01 | batch 300/2170 | loss 4.4142 | top1 7.29% | top5 17.85% | 51.0 img/s\n",
      "Epoch 01 | batch 300/2170 | loss 4.4142 | top1 7.29% | top5 17.85% | 51.0 img/s\n",
      "Epoch 01 | batch 350/2170 | loss 4.3462 | top1 7.71% | top5 18.46% | 51.3 img/s\n",
      "Epoch 01 | batch 350/2170 | loss 4.3462 | top1 7.71% | top5 18.46% | 51.3 img/s\n",
      "Epoch 01 | batch 400/2170 | loss 4.2953 | top1 7.89% | top5 18.56% | 51.5 img/s\n",
      "Epoch 01 | batch 400/2170 | loss 4.2953 | top1 7.89% | top5 18.56% | 51.5 img/s\n",
      "Epoch 01 | batch 450/2170 | loss 4.2379 | top1 8.69% | top5 19.86% | 51.4 img/s\n",
      "Epoch 01 | batch 450/2170 | loss 4.2379 | top1 8.69% | top5 19.86% | 51.4 img/s\n",
      "Epoch 01 | batch 500/2170 | loss 4.1849 | top1 9.51% | top5 20.98% | 51.4 img/s\n",
      "Epoch 01 | batch 500/2170 | loss 4.1849 | top1 9.51% | top5 20.98% | 51.4 img/s\n",
      "Epoch 01 | batch 550/2170 | loss 4.1483 | top1 10.34% | top5 22.36% | 51.3 img/s\n",
      "Epoch 01 | batch 550/2170 | loss 4.1483 | top1 10.34% | top5 22.36% | 51.3 img/s\n",
      "Epoch 01 | batch 600/2170 | loss 4.1100 | top1 10.80% | top5 23.19% | 51.0 img/s\n",
      "Epoch 01 | batch 600/2170 | loss 4.1100 | top1 10.80% | top5 23.19% | 51.0 img/s\n",
      "Epoch 01 | batch 650/2170 | loss 4.0780 | top1 11.09% | top5 23.69% | 50.8 img/s\n",
      "Epoch 01 | batch 650/2170 | loss 4.0780 | top1 11.09% | top5 23.69% | 50.8 img/s\n",
      "Epoch 01 | batch 700/2170 | loss 4.0438 | top1 11.32% | top5 24.17% | 50.5 img/s\n",
      "Epoch 01 | batch 700/2170 | loss 4.0438 | top1 11.32% | top5 24.17% | 50.5 img/s\n",
      "Epoch 01 | batch 750/2170 | loss 4.0171 | top1 12.01% | top5 25.14% | 50.3 img/s\n",
      "Epoch 01 | batch 750/2170 | loss 4.0171 | top1 12.01% | top5 25.14% | 50.3 img/s\n",
      "Epoch 01 | batch 800/2170 | loss 3.9901 | top1 12.04% | top5 25.14% | 50.2 img/s\n",
      "Epoch 01 | batch 800/2170 | loss 3.9901 | top1 12.04% | top5 25.14% | 50.2 img/s\n",
      "Epoch 01 | batch 850/2170 | loss 3.9624 | top1 12.24% | top5 25.62% | 50.0 img/s\n",
      "Epoch 01 | batch 850/2170 | loss 3.9624 | top1 12.24% | top5 25.62% | 50.0 img/s\n",
      "Epoch 01 | batch 900/2170 | loss 3.9387 | top1 12.39% | top5 26.12% | 50.0 img/s\n",
      "Epoch 01 | batch 900/2170 | loss 3.9387 | top1 12.39% | top5 26.12% | 50.0 img/s\n",
      "Epoch 01 | batch 950/2170 | loss 3.9145 | top1 12.80% | top5 26.72% | 49.9 img/s\n",
      "Epoch 01 | batch 950/2170 | loss 3.9145 | top1 12.80% | top5 26.72% | 49.9 img/s\n",
      "Epoch 01 | batch 1000/2170 | loss 3.8921 | top1 12.86% | top5 26.80% | 50.0 img/s\n",
      "Epoch 01 | batch 1000/2170 | loss 3.8921 | top1 12.86% | top5 26.80% | 50.0 img/s\n",
      "Epoch 01 | batch 1050/2170 | loss 3.8689 | top1 13.05% | top5 26.99% | 49.9 img/s\n",
      "Epoch 01 | batch 1050/2170 | loss 3.8689 | top1 13.05% | top5 26.99% | 49.9 img/s\n",
      "Epoch 01 | batch 1100/2170 | loss 3.8496 | top1 13.24% | top5 27.29% | 49.8 img/s\n",
      "Epoch 01 | batch 1100/2170 | loss 3.8496 | top1 13.24% | top5 27.29% | 49.8 img/s\n",
      "Epoch 01 | batch 1150/2170 | loss 3.8293 | top1 13.61% | top5 27.83% | 49.7 img/s\n",
      "Epoch 01 | batch 1150/2170 | loss 3.8293 | top1 13.61% | top5 27.83% | 49.7 img/s\n",
      "Epoch 01 | batch 1200/2170 | loss 3.8130 | top1 13.90% | top5 28.11% | 49.7 img/s\n",
      "Epoch 01 | batch 1200/2170 | loss 3.8130 | top1 13.90% | top5 28.11% | 49.7 img/s\n",
      "Epoch 01 | batch 1250/2170 | loss 3.7982 | top1 13.95% | top5 28.22% | 49.6 img/s\n",
      "Epoch 01 | batch 1250/2170 | loss 3.7982 | top1 13.95% | top5 28.22% | 49.6 img/s\n",
      "Epoch 01 | batch 1300/2170 | loss 3.7819 | top1 14.11% | top5 28.55% | 49.6 img/s\n",
      "Epoch 01 | batch 1300/2170 | loss 3.7819 | top1 14.11% | top5 28.55% | 49.6 img/s\n",
      "Epoch 01 | batch 1350/2170 | loss 3.7622 | top1 14.31% | top5 28.92% | 49.5 img/s\n",
      "Epoch 01 | batch 1350/2170 | loss 3.7622 | top1 14.31% | top5 28.92% | 49.5 img/s\n",
      "Epoch 01 | batch 1400/2170 | loss 3.7430 | top1 14.55% | top5 29.32% | 49.4 img/s\n",
      "Epoch 01 | batch 1400/2170 | loss 3.7430 | top1 14.55% | top5 29.32% | 49.4 img/s\n",
      "Epoch 01 | batch 1450/2170 | loss 3.7318 | top1 14.65% | top5 29.47% | 49.5 img/s\n",
      "Epoch 01 | batch 1450/2170 | loss 3.7318 | top1 14.65% | top5 29.47% | 49.5 img/s\n",
      "Epoch 01 | batch 1500/2170 | loss 3.7204 | top1 14.72% | top5 29.69% | 49.5 img/s\n",
      "Epoch 01 | batch 1500/2170 | loss 3.7204 | top1 14.72% | top5 29.69% | 49.5 img/s\n",
      "Epoch 01 | batch 1550/2170 | loss 3.7097 | top1 14.87% | top5 29.99% | 49.6 img/s\n",
      "Epoch 01 | batch 1550/2170 | loss 3.7097 | top1 14.87% | top5 29.99% | 49.6 img/s\n",
      "Epoch 01 | batch 1600/2170 | loss 3.7017 | top1 15.07% | top5 30.29% | 49.6 img/s\n",
      "Epoch 01 | batch 1600/2170 | loss 3.7017 | top1 15.07% | top5 30.29% | 49.6 img/s\n",
      "Epoch 01 | batch 1650/2170 | loss 3.6922 | top1 15.22% | top5 30.52% | 49.7 img/s\n",
      "Epoch 01 | batch 1650/2170 | loss 3.6922 | top1 15.22% | top5 30.52% | 49.7 img/s\n",
      "Epoch 01 | batch 1700/2170 | loss 3.6771 | top1 15.32% | top5 30.54% | 49.8 img/s\n",
      "Epoch 01 | batch 1700/2170 | loss 3.6771 | top1 15.32% | top5 30.54% | 49.8 img/s\n",
      "Epoch 01 | batch 1750/2170 | loss 3.6659 | top1 15.41% | top5 30.69% | 49.9 img/s\n",
      "Epoch 01 | batch 1750/2170 | loss 3.6659 | top1 15.41% | top5 30.69% | 49.9 img/s\n",
      "Epoch 01 | batch 1800/2170 | loss 3.6532 | top1 15.63% | top5 30.96% | 49.9 img/s\n",
      "Epoch 01 | batch 1800/2170 | loss 3.6532 | top1 15.63% | top5 30.96% | 49.9 img/s\n",
      "Epoch 01 | batch 1850/2170 | loss 3.6408 | top1 15.85% | top5 31.22% | 49.9 img/s\n",
      "Epoch 01 | batch 1850/2170 | loss 3.6408 | top1 15.85% | top5 31.22% | 49.9 img/s\n",
      "Epoch 01 | batch 1900/2170 | loss 3.6307 | top1 15.98% | top5 31.39% | 49.8 img/s\n",
      "Epoch 01 | batch 1900/2170 | loss 3.6307 | top1 15.98% | top5 31.39% | 49.8 img/s\n",
      "Epoch 01 | batch 1950/2170 | loss 3.6224 | top1 16.21% | top5 31.71% | 49.8 img/s\n",
      "Epoch 01 | batch 1950/2170 | loss 3.6224 | top1 16.21% | top5 31.71% | 49.8 img/s\n",
      "Epoch 01 | batch 2000/2170 | loss 3.6145 | top1 16.38% | top5 31.98% | 49.8 img/s\n",
      "Epoch 01 | batch 2000/2170 | loss 3.6145 | top1 16.38% | top5 31.98% | 49.8 img/s\n",
      "Epoch 01 | batch 2050/2170 | loss 3.6071 | top1 16.42% | top5 31.98% | 49.8 img/s\n",
      "Epoch 01 | batch 2050/2170 | loss 3.6071 | top1 16.42% | top5 31.98% | 49.8 img/s\n",
      "Epoch 01 | batch 2100/2170 | loss 3.6010 | top1 16.49% | top5 32.04% | 49.8 img/s\n",
      "Epoch 01 | batch 2100/2170 | loss 3.6010 | top1 16.49% | top5 32.04% | 49.8 img/s\n",
      "Epoch 01 | batch 2150/2170 | loss 3.5956 | top1 16.57% | top5 32.10% | 49.9 img/s\n",
      "Epoch 01 | batch 2150/2170 | loss 3.5956 | top1 16.57% | top5 32.10% | 49.9 img/s\n",
      "Epoch 01 | batch 2170/2170 | loss 3.5944 | top1 16.58% | top5 32.13% | 49.9 img/s\n",
      "Epoch 01 | batch 2170/2170 | loss 3.5944 | top1 16.58% | top5 32.13% | 49.9 img/s\n",
      "\n",
      "Epoch 01/50 | 1000.2s\n",
      "  Train(mixup): loss 3.5944 | top1 16.58% | top5 32.13%\n",
      "  Train(clean): loss 2.4882 | top1 56.18% | top5 82.23%\n",
      "  Val:          loss 2.5512 | top1 53.34% | top5 80.32%\n",
      "\n",
      "Epoch 01/50 | 1000.2s\n",
      "  Train(mixup): loss 3.5944 | top1 16.58% | top5 32.13%\n",
      "  Train(clean): loss 2.4882 | top1 56.18% | top5 82.23%\n",
      "  Val:          loss 2.5512 | top1 53.34% | top5 80.32%\n",
      "  ✓ Saved new best: d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\\best_efficientnet_b3.pth (val Top-1=53.34%)\n",
      "  ✓ Saved new best: d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\\best_efficientnet_b3.pth (val Top-1=53.34%)\n",
      "Epoch 02 | batch 50/2170 | loss 3.1711 | top1 25.62% | top5 41.75% | 53.0 img/s\n",
      "Epoch 02 | batch 50/2170 | loss 3.1711 | top1 25.62% | top5 41.75% | 53.0 img/s\n",
      "Epoch 02 | batch 100/2170 | loss 3.2033 | top1 22.88% | top5 39.69% | 53.2 img/s\n",
      "Epoch 02 | batch 100/2170 | loss 3.2033 | top1 22.88% | top5 39.69% | 53.2 img/s\n",
      "Epoch 02 | batch 150/2170 | loss 3.1747 | top1 22.38% | top5 39.29% | 53.5 img/s\n",
      "Epoch 02 | batch 150/2170 | loss 3.1747 | top1 22.38% | top5 39.29% | 53.5 img/s\n",
      "Epoch 02 | batch 200/2170 | loss 3.1892 | top1 21.62% | top5 38.22% | 53.5 img/s\n",
      "Epoch 02 | batch 200/2170 | loss 3.1892 | top1 21.62% | top5 38.22% | 53.5 img/s\n",
      "Epoch 02 | batch 250/2170 | loss 3.1937 | top1 21.73% | top5 38.25% | 53.5 img/s\n",
      "Epoch 02 | batch 250/2170 | loss 3.1937 | top1 21.73% | top5 38.25% | 53.5 img/s\n",
      "Epoch 02 | batch 300/2170 | loss 3.1741 | top1 22.33% | top5 38.88% | 53.4 img/s\n",
      "Epoch 02 | batch 300/2170 | loss 3.1741 | top1 22.33% | top5 38.88% | 53.4 img/s\n",
      "Epoch 02 | batch 350/2170 | loss 3.1609 | top1 22.16% | top5 38.91% | 53.2 img/s\n",
      "Epoch 02 | batch 350/2170 | loss 3.1609 | top1 22.16% | top5 38.91% | 53.2 img/s\n",
      "Epoch 02 | batch 400/2170 | loss 3.1669 | top1 21.77% | top5 38.52% | 53.3 img/s\n",
      "Epoch 02 | batch 400/2170 | loss 3.1669 | top1 21.77% | top5 38.52% | 53.3 img/s\n",
      "Epoch 02 | batch 450/2170 | loss 3.1620 | top1 21.65% | top5 38.32% | 53.3 img/s\n",
      "Epoch 02 | batch 450/2170 | loss 3.1620 | top1 21.65% | top5 38.32% | 53.3 img/s\n",
      "Epoch 02 | batch 500/2170 | loss 3.1651 | top1 21.89% | top5 38.74% | 53.2 img/s\n",
      "Epoch 02 | batch 500/2170 | loss 3.1651 | top1 21.89% | top5 38.74% | 53.2 img/s\n",
      "Epoch 02 | batch 550/2170 | loss 3.1749 | top1 21.83% | top5 38.88% | 53.0 img/s\n",
      "Epoch 02 | batch 550/2170 | loss 3.1749 | top1 21.83% | top5 38.88% | 53.0 img/s\n",
      "Epoch 02 | batch 600/2170 | loss 3.1565 | top1 22.06% | top5 39.10% | 52.8 img/s\n",
      "Epoch 02 | batch 600/2170 | loss 3.1565 | top1 22.06% | top5 39.10% | 52.8 img/s\n",
      "Epoch 02 | batch 650/2170 | loss 3.1627 | top1 22.06% | top5 39.11% | 52.6 img/s\n",
      "Epoch 02 | batch 650/2170 | loss 3.1627 | top1 22.06% | top5 39.11% | 52.6 img/s\n",
      "Epoch 02 | batch 700/2170 | loss 3.1608 | top1 21.94% | top5 38.93% | 52.3 img/s\n",
      "Epoch 02 | batch 700/2170 | loss 3.1608 | top1 21.94% | top5 38.93% | 52.3 img/s\n",
      "Epoch 02 | batch 750/2170 | loss 3.1549 | top1 21.73% | top5 38.62% | 51.9 img/s\n",
      "Epoch 02 | batch 750/2170 | loss 3.1549 | top1 21.73% | top5 38.62% | 51.9 img/s\n",
      "Epoch 02 | batch 800/2170 | loss 3.1630 | top1 21.50% | top5 38.28% | 51.9 img/s\n",
      "Epoch 02 | batch 800/2170 | loss 3.1630 | top1 21.50% | top5 38.28% | 51.9 img/s\n",
      "Epoch 02 | batch 850/2170 | loss 3.1553 | top1 21.62% | top5 38.34% | 51.8 img/s\n",
      "Epoch 02 | batch 850/2170 | loss 3.1553 | top1 21.62% | top5 38.34% | 51.8 img/s\n",
      "Epoch 02 | batch 900/2170 | loss 3.1570 | top1 21.43% | top5 37.91% | 51.7 img/s\n",
      "Epoch 02 | batch 900/2170 | loss 3.1570 | top1 21.43% | top5 37.91% | 51.7 img/s\n",
      "Epoch 02 | batch 950/2170 | loss 3.1504 | top1 21.80% | top5 38.48% | 51.6 img/s\n",
      "Epoch 02 | batch 950/2170 | loss 3.1504 | top1 21.80% | top5 38.48% | 51.6 img/s\n",
      "Epoch 02 | batch 1000/2170 | loss 3.1508 | top1 21.75% | top5 38.38% | 51.6 img/s\n",
      "Epoch 02 | batch 1000/2170 | loss 3.1508 | top1 21.75% | top5 38.38% | 51.6 img/s\n",
      "Epoch 02 | batch 1050/2170 | loss 3.1448 | top1 21.99% | top5 38.62% | 51.5 img/s\n",
      "Epoch 02 | batch 1050/2170 | loss 3.1448 | top1 21.99% | top5 38.62% | 51.5 img/s\n",
      "Epoch 02 | batch 1100/2170 | loss 3.1395 | top1 22.20% | top5 38.97% | 51.5 img/s\n",
      "Epoch 02 | batch 1100/2170 | loss 3.1395 | top1 22.20% | top5 38.97% | 51.5 img/s\n",
      "Epoch 02 | batch 1150/2170 | loss 3.1355 | top1 22.35% | top5 39.19% | 51.4 img/s\n",
      "Epoch 02 | batch 1150/2170 | loss 3.1355 | top1 22.35% | top5 39.19% | 51.4 img/s\n",
      "Epoch 02 | batch 1200/2170 | loss 3.1371 | top1 22.49% | top5 39.41% | 51.4 img/s\n",
      "Epoch 02 | batch 1200/2170 | loss 3.1371 | top1 22.49% | top5 39.41% | 51.4 img/s\n",
      "Epoch 02 | batch 1250/2170 | loss 3.1390 | top1 22.45% | top5 39.49% | 51.4 img/s\n",
      "Epoch 02 | batch 1250/2170 | loss 3.1390 | top1 22.45% | top5 39.49% | 51.4 img/s\n",
      "Epoch 02 | batch 1300/2170 | loss 3.1414 | top1 22.36% | top5 39.28% | 51.4 img/s\n",
      "Epoch 02 | batch 1300/2170 | loss 3.1414 | top1 22.36% | top5 39.28% | 51.4 img/s\n",
      "Epoch 02 | batch 1350/2170 | loss 3.1378 | top1 22.31% | top5 39.12% | 51.4 img/s\n",
      "Epoch 02 | batch 1350/2170 | loss 3.1378 | top1 22.31% | top5 39.12% | 51.4 img/s\n",
      "Epoch 02 | batch 1400/2170 | loss 3.1412 | top1 22.41% | top5 39.26% | 51.4 img/s\n",
      "Epoch 02 | batch 1400/2170 | loss 3.1412 | top1 22.41% | top5 39.26% | 51.4 img/s\n",
      "Epoch 02 | batch 1450/2170 | loss 3.1453 | top1 22.30% | top5 39.12% | 51.2 img/s\n",
      "Epoch 02 | batch 1450/2170 | loss 3.1453 | top1 22.30% | top5 39.12% | 51.2 img/s\n",
      "Epoch 02 | batch 1500/2170 | loss 3.1425 | top1 22.40% | top5 39.17% | 51.2 img/s\n",
      "Epoch 02 | batch 1500/2170 | loss 3.1425 | top1 22.40% | top5 39.17% | 51.2 img/s\n",
      "Epoch 02 | batch 1550/2170 | loss 3.1372 | top1 22.31% | top5 38.95% | 51.2 img/s\n",
      "Epoch 02 | batch 1550/2170 | loss 3.1372 | top1 22.31% | top5 38.95% | 51.2 img/s\n",
      "Epoch 02 | batch 1600/2170 | loss 3.1318 | top1 22.37% | top5 38.96% | 51.2 img/s\n",
      "Epoch 02 | batch 1600/2170 | loss 3.1318 | top1 22.37% | top5 38.96% | 51.2 img/s\n",
      "Epoch 02 | batch 1650/2170 | loss 3.1311 | top1 22.45% | top5 39.02% | 51.2 img/s\n",
      "Epoch 02 | batch 1650/2170 | loss 3.1311 | top1 22.45% | top5 39.02% | 51.2 img/s\n",
      "Epoch 02 | batch 1700/2170 | loss 3.1317 | top1 22.62% | top5 39.18% | 51.2 img/s\n",
      "Epoch 02 | batch 1700/2170 | loss 3.1317 | top1 22.62% | top5 39.18% | 51.2 img/s\n",
      "Epoch 02 | batch 1750/2170 | loss 3.1298 | top1 22.61% | top5 39.12% | 51.2 img/s\n",
      "Epoch 02 | batch 1750/2170 | loss 3.1298 | top1 22.61% | top5 39.12% | 51.2 img/s\n",
      "Epoch 02 | batch 1800/2170 | loss 3.1244 | top1 22.66% | top5 39.12% | 51.2 img/s\n",
      "Epoch 02 | batch 1800/2170 | loss 3.1244 | top1 22.66% | top5 39.12% | 51.2 img/s\n",
      "Epoch 02 | batch 1850/2170 | loss 3.1244 | top1 22.72% | top5 39.16% | 51.3 img/s\n",
      "Epoch 02 | batch 1850/2170 | loss 3.1244 | top1 22.72% | top5 39.16% | 51.3 img/s\n",
      "Epoch 02 | batch 1900/2170 | loss 3.1218 | top1 22.79% | top5 39.26% | 51.3 img/s\n",
      "Epoch 02 | batch 1900/2170 | loss 3.1218 | top1 22.79% | top5 39.26% | 51.3 img/s\n",
      "Epoch 02 | batch 1950/2170 | loss 3.1228 | top1 22.82% | top5 39.30% | 51.3 img/s\n",
      "Epoch 02 | batch 1950/2170 | loss 3.1228 | top1 22.82% | top5 39.30% | 51.3 img/s\n",
      "Epoch 02 | batch 2000/2170 | loss 3.1215 | top1 22.79% | top5 39.26% | 51.3 img/s\n",
      "Epoch 02 | batch 2000/2170 | loss 3.1215 | top1 22.79% | top5 39.26% | 51.3 img/s\n",
      "Epoch 02 | batch 2050/2170 | loss 3.1217 | top1 22.74% | top5 39.21% | 51.3 img/s\n",
      "Epoch 02 | batch 2050/2170 | loss 3.1217 | top1 22.74% | top5 39.21% | 51.3 img/s\n",
      "Epoch 02 | batch 2100/2170 | loss 3.1224 | top1 22.80% | top5 39.34% | 51.4 img/s\n",
      "Epoch 02 | batch 2100/2170 | loss 3.1224 | top1 22.80% | top5 39.34% | 51.4 img/s\n",
      "Epoch 02 | batch 2150/2170 | loss 3.1233 | top1 22.69% | top5 39.17% | 51.5 img/s\n",
      "Epoch 02 | batch 2150/2170 | loss 3.1233 | top1 22.69% | top5 39.17% | 51.5 img/s\n",
      "Epoch 02 | batch 2170/2170 | loss 3.1226 | top1 22.64% | top5 39.16% | 51.5 img/s\n",
      "Epoch 02 | batch 2170/2170 | loss 3.1226 | top1 22.64% | top5 39.16% | 51.5 img/s\n",
      "\n",
      "Epoch 02/50 | 970.5s\n",
      "  Train(mixup): loss 3.1226 | top1 22.64% | top5 39.16%\n",
      "  Train(clean): loss 2.2310 | top1 61.26% | top5 85.40%\n",
      "  Val:          loss 2.3322 | top1 58.39% | top5 82.83%\n",
      "\n",
      "Epoch 02/50 | 970.5s\n",
      "  Train(mixup): loss 3.1226 | top1 22.64% | top5 39.16%\n",
      "  Train(clean): loss 2.2310 | top1 61.26% | top5 85.40%\n",
      "  Val:          loss 2.3322 | top1 58.39% | top5 82.83%\n",
      "  ✓ Saved new best: d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\\best_efficientnet_b3.pth (val Top-1=58.39%)\n",
      "  ✓ Saved new best: d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\\best_efficientnet_b3.pth (val Top-1=58.39%)\n",
      "Epoch 03 | batch 50/2170 | loss 3.1703 | top1 22.25% | top5 38.50% | 52.2 img/s\n",
      "Epoch 03 | batch 50/2170 | loss 3.1703 | top1 22.25% | top5 38.50% | 52.2 img/s\n",
      "Epoch 03 | batch 100/2170 | loss 3.1511 | top1 22.94% | top5 38.81% | 51.9 img/s\n",
      "Epoch 03 | batch 100/2170 | loss 3.1511 | top1 22.94% | top5 38.81% | 51.9 img/s\n",
      "Epoch 03 | batch 150/2170 | loss 3.1397 | top1 22.38% | top5 38.67% | 52.0 img/s\n",
      "Epoch 03 | batch 150/2170 | loss 3.1397 | top1 22.38% | top5 38.67% | 52.0 img/s\n",
      "Epoch 03 | batch 200/2170 | loss 3.1014 | top1 23.22% | top5 40.47% | 52.5 img/s\n",
      "Epoch 03 | batch 200/2170 | loss 3.1014 | top1 23.22% | top5 40.47% | 52.5 img/s\n",
      "Epoch 03 | batch 250/2170 | loss 3.1180 | top1 22.50% | top5 39.35% | 52.4 img/s\n",
      "Epoch 03 | batch 250/2170 | loss 3.1180 | top1 22.50% | top5 39.35% | 52.4 img/s\n",
      "Epoch 03 | batch 300/2170 | loss 3.1032 | top1 23.25% | top5 40.23% | 52.4 img/s\n",
      "Epoch 03 | batch 300/2170 | loss 3.1032 | top1 23.25% | top5 40.23% | 52.4 img/s\n",
      "Epoch 03 | batch 350/2170 | loss 3.1020 | top1 22.71% | top5 39.46% | 52.4 img/s\n",
      "Epoch 03 | batch 350/2170 | loss 3.1020 | top1 22.71% | top5 39.46% | 52.4 img/s\n",
      "Epoch 03 | batch 400/2170 | loss 3.1003 | top1 22.92% | top5 39.67% | 52.2 img/s\n",
      "Epoch 03 | batch 400/2170 | loss 3.1003 | top1 22.92% | top5 39.67% | 52.2 img/s\n",
      "Epoch 03 | batch 450/2170 | loss 3.0976 | top1 23.14% | top5 39.57% | 52.1 img/s\n",
      "Epoch 03 | batch 450/2170 | loss 3.0976 | top1 23.14% | top5 39.57% | 52.1 img/s\n",
      "Epoch 03 | batch 500/2170 | loss 3.0952 | top1 23.60% | top5 40.60% | 51.9 img/s\n",
      "Epoch 03 | batch 500/2170 | loss 3.0952 | top1 23.60% | top5 40.60% | 51.9 img/s\n",
      "Epoch 03 | batch 550/2170 | loss 3.1019 | top1 23.78% | top5 41.12% | 51.8 img/s\n",
      "Epoch 03 | batch 550/2170 | loss 3.1019 | top1 23.78% | top5 41.12% | 51.8 img/s\n",
      "Epoch 03 | batch 600/2170 | loss 3.1009 | top1 24.29% | top5 41.85% | 51.7 img/s\n",
      "Epoch 03 | batch 600/2170 | loss 3.1009 | top1 24.29% | top5 41.85% | 51.7 img/s\n",
      "Epoch 03 | batch 650/2170 | loss 3.1095 | top1 23.80% | top5 41.24% | 51.7 img/s\n",
      "Epoch 03 | batch 650/2170 | loss 3.1095 | top1 23.80% | top5 41.24% | 51.7 img/s\n",
      "Epoch 03 | batch 700/2170 | loss 3.1087 | top1 24.11% | top5 41.64% | 51.6 img/s\n",
      "Epoch 03 | batch 700/2170 | loss 3.1087 | top1 24.11% | top5 41.64% | 51.6 img/s\n",
      "Epoch 03 | batch 750/2170 | loss 3.1199 | top1 23.80% | top5 41.12% | 51.5 img/s\n",
      "Epoch 03 | batch 750/2170 | loss 3.1199 | top1 23.80% | top5 41.12% | 51.5 img/s\n",
      "Epoch 03 | batch 800/2170 | loss 3.1132 | top1 24.03% | top5 41.53% | 51.4 img/s\n",
      "Epoch 03 | batch 800/2170 | loss 3.1132 | top1 24.03% | top5 41.53% | 51.4 img/s\n",
      "Epoch 03 | batch 850/2170 | loss 3.1111 | top1 24.12% | top5 41.68% | 51.3 img/s\n",
      "Epoch 03 | batch 850/2170 | loss 3.1111 | top1 24.12% | top5 41.68% | 51.3 img/s\n",
      "Epoch 03 | batch 900/2170 | loss 3.1025 | top1 24.29% | top5 41.71% | 51.3 img/s\n",
      "Epoch 03 | batch 900/2170 | loss 3.1025 | top1 24.29% | top5 41.71% | 51.3 img/s\n",
      "Epoch 03 | batch 950/2170 | loss 3.1027 | top1 24.36% | top5 41.66% | 51.3 img/s\n",
      "Epoch 03 | batch 950/2170 | loss 3.1027 | top1 24.36% | top5 41.66% | 51.3 img/s\n",
      "Epoch 03 | batch 1000/2170 | loss 3.0971 | top1 24.42% | top5 41.51% | 51.2 img/s\n",
      "Epoch 03 | batch 1000/2170 | loss 3.0971 | top1 24.42% | top5 41.51% | 51.2 img/s\n",
      "Epoch 03 | batch 1050/2170 | loss 3.0990 | top1 24.04% | top5 41.04% | 51.2 img/s\n",
      "Epoch 03 | batch 1050/2170 | loss 3.0990 | top1 24.04% | top5 41.04% | 51.2 img/s\n",
      "Epoch 03 | batch 1100/2170 | loss 3.0916 | top1 24.32% | top5 41.43% | 51.1 img/s\n",
      "Epoch 03 | batch 1100/2170 | loss 3.0916 | top1 24.32% | top5 41.43% | 51.1 img/s\n",
      "Epoch 03 | batch 1150/2170 | loss 3.0880 | top1 24.33% | top5 41.35% | 51.1 img/s\n",
      "Epoch 03 | batch 1150/2170 | loss 3.0880 | top1 24.33% | top5 41.35% | 51.1 img/s\n",
      "Epoch 03 | batch 1200/2170 | loss 3.0887 | top1 24.23% | top5 41.22% | 51.0 img/s\n",
      "Epoch 03 | batch 1200/2170 | loss 3.0887 | top1 24.23% | top5 41.22% | 51.0 img/s\n",
      "Epoch 03 | batch 1250/2170 | loss 3.0961 | top1 24.07% | top5 41.01% | 51.0 img/s\n",
      "Epoch 03 | batch 1250/2170 | loss 3.0961 | top1 24.07% | top5 41.01% | 51.0 img/s\n",
      "Epoch 03 | batch 1300/2170 | loss 3.0958 | top1 23.86% | top5 40.72% | 51.0 img/s\n",
      "Epoch 03 | batch 1300/2170 | loss 3.0958 | top1 23.86% | top5 40.72% | 51.0 img/s\n",
      "Epoch 03 | batch 1350/2170 | loss 3.0969 | top1 24.00% | top5 40.84% | 51.0 img/s\n",
      "Epoch 03 | batch 1350/2170 | loss 3.0969 | top1 24.00% | top5 40.84% | 51.0 img/s\n",
      "Epoch 03 | batch 1400/2170 | loss 3.0921 | top1 23.99% | top5 40.79% | 51.0 img/s\n",
      "Epoch 03 | batch 1400/2170 | loss 3.0921 | top1 23.99% | top5 40.79% | 51.0 img/s\n",
      "Epoch 03 | batch 1450/2170 | loss 3.0922 | top1 23.91% | top5 40.75% | 51.0 img/s\n",
      "Epoch 03 | batch 1450/2170 | loss 3.0922 | top1 23.91% | top5 40.75% | 51.0 img/s\n",
      "Epoch 03 | batch 1500/2170 | loss 3.0914 | top1 23.88% | top5 40.59% | 50.9 img/s\n",
      "Epoch 03 | batch 1500/2170 | loss 3.0914 | top1 23.88% | top5 40.59% | 50.9 img/s\n",
      "Epoch 03 | batch 1550/2170 | loss 3.0907 | top1 23.83% | top5 40.56% | 50.9 img/s\n",
      "Epoch 03 | batch 1550/2170 | loss 3.0907 | top1 23.83% | top5 40.56% | 50.9 img/s\n",
      "Epoch 03 | batch 1600/2170 | loss 3.0908 | top1 23.84% | top5 40.57% | 50.9 img/s\n",
      "Epoch 03 | batch 1600/2170 | loss 3.0908 | top1 23.84% | top5 40.57% | 50.9 img/s\n",
      "Epoch 03 | batch 1650/2170 | loss 3.0921 | top1 23.92% | top5 40.57% | 50.9 img/s\n",
      "Epoch 03 | batch 1650/2170 | loss 3.0921 | top1 23.92% | top5 40.57% | 50.9 img/s\n",
      "Epoch 03 | batch 1700/2170 | loss 3.0959 | top1 23.94% | top5 40.60% | 50.9 img/s\n",
      "Epoch 03 | batch 1700/2170 | loss 3.0959 | top1 23.94% | top5 40.60% | 50.9 img/s\n",
      "Epoch 03 | batch 1750/2170 | loss 3.1008 | top1 23.91% | top5 40.55% | 50.9 img/s\n",
      "Epoch 03 | batch 1750/2170 | loss 3.1008 | top1 23.91% | top5 40.55% | 50.9 img/s\n",
      "Epoch 03 | batch 1800/2170 | loss 3.1011 | top1 23.82% | top5 40.39% | 50.9 img/s\n",
      "Epoch 03 | batch 1800/2170 | loss 3.1011 | top1 23.82% | top5 40.39% | 50.9 img/s\n",
      "Epoch 03 | batch 1850/2170 | loss 3.1027 | top1 23.87% | top5 40.51% | 50.9 img/s\n",
      "Epoch 03 | batch 1850/2170 | loss 3.1027 | top1 23.87% | top5 40.51% | 50.9 img/s\n",
      "Epoch 03 | batch 1900/2170 | loss 3.1001 | top1 23.80% | top5 40.38% | 50.9 img/s\n",
      "Epoch 03 | batch 1900/2170 | loss 3.1001 | top1 23.80% | top5 40.38% | 50.9 img/s\n",
      "Epoch 03 | batch 1950/2170 | loss 3.0970 | top1 23.84% | top5 40.46% | 50.9 img/s\n",
      "Epoch 03 | batch 1950/2170 | loss 3.0970 | top1 23.84% | top5 40.46% | 50.9 img/s\n",
      "Epoch 03 | batch 2000/2170 | loss 3.0954 | top1 23.69% | top5 40.24% | 50.8 img/s\n",
      "Epoch 03 | batch 2000/2170 | loss 3.0954 | top1 23.69% | top5 40.24% | 50.8 img/s\n",
      "Epoch 03 | batch 2050/2170 | loss 3.0948 | top1 23.76% | top5 40.30% | 50.8 img/s\n",
      "Epoch 03 | batch 2050/2170 | loss 3.0948 | top1 23.76% | top5 40.30% | 50.8 img/s\n",
      "Epoch 03 | batch 2100/2170 | loss 3.0968 | top1 23.59% | top5 40.09% | 50.8 img/s\n",
      "Epoch 03 | batch 2100/2170 | loss 3.0968 | top1 23.59% | top5 40.09% | 50.8 img/s\n",
      "Epoch 03 | batch 2150/2170 | loss 3.0958 | top1 23.50% | top5 39.92% | 50.9 img/s\n",
      "Epoch 03 | batch 2150/2170 | loss 3.0958 | top1 23.50% | top5 39.92% | 50.9 img/s\n",
      "Epoch 03 | batch 2170/2170 | loss 3.0970 | top1 23.44% | top5 39.84% | 50.9 img/s\n",
      "Epoch 03 | batch 2170/2170 | loss 3.0970 | top1 23.44% | top5 39.84% | 50.9 img/s\n",
      "\n",
      "Epoch 03/50 | 995.9s\n",
      "  Train(mixup): loss 3.0970 | top1 23.44% | top5 39.84%\n",
      "  Train(clean): loss 2.1843 | top1 63.07% | top5 86.68%\n",
      "  Val:          loss 2.2801 | top1 60.07% | top5 83.71%\n",
      "\n",
      "Epoch 03/50 | 995.9s\n",
      "  Train(mixup): loss 3.0970 | top1 23.44% | top5 39.84%\n",
      "  Train(clean): loss 2.1843 | top1 63.07% | top5 86.68%\n",
      "  Val:          loss 2.2801 | top1 60.07% | top5 83.71%\n",
      "  ✓ Saved new best: d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\\best_efficientnet_b3.pth (val Top-1=60.07%)\n",
      "\n",
      "=== Unfreezing backbone at epoch 4 ===\n",
      "  ✓ Saved new best: d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\\best_efficientnet_b3.pth (val Top-1=60.07%)\n",
      "\n",
      "=== Unfreezing backbone at epoch 4 ===\n",
      "Epoch 04 | batch 50/2170 | loss 3.0509 | top1 30.50% | top5 49.00% | 12.0 img/s\n",
      "Epoch 04 | batch 50/2170 | loss 3.0509 | top1 30.50% | top5 49.00% | 12.0 img/s\n",
      "Epoch 04 | batch 100/2170 | loss 3.0318 | top1 30.31% | top5 48.75% | 12.2 img/s\n",
      "Epoch 04 | batch 100/2170 | loss 3.0318 | top1 30.31% | top5 48.75% | 12.2 img/s\n",
      "Epoch 04 | batch 150/2170 | loss 2.9486 | top1 30.29% | top5 48.83% | 12.1 img/s\n",
      "Epoch 04 | batch 150/2170 | loss 2.9486 | top1 30.29% | top5 48.83% | 12.1 img/s\n",
      "Epoch 04 | batch 200/2170 | loss 2.8696 | top1 29.00% | top5 46.25% | 12.2 img/s\n",
      "Epoch 04 | batch 200/2170 | loss 2.8696 | top1 29.00% | top5 46.25% | 12.2 img/s\n",
      "Epoch 04 | batch 250/2170 | loss 2.8285 | top1 30.35% | top5 47.10% | 12.2 img/s\n",
      "Epoch 04 | batch 250/2170 | loss 2.8285 | top1 30.35% | top5 47.10% | 12.2 img/s\n",
      "Epoch 04 | batch 300/2170 | loss 2.8056 | top1 29.67% | top5 45.69% | 12.2 img/s\n",
      "Epoch 04 | batch 300/2170 | loss 2.8056 | top1 29.67% | top5 45.69% | 12.2 img/s\n",
      "Epoch 04 | batch 350/2170 | loss 2.8023 | top1 29.45% | top5 45.25% | 12.2 img/s\n",
      "Epoch 04 | batch 350/2170 | loss 2.8023 | top1 29.45% | top5 45.25% | 12.2 img/s\n",
      "Epoch 04 | batch 400/2170 | loss 2.7844 | top1 30.19% | top5 46.39% | 12.2 img/s\n",
      "Epoch 04 | batch 400/2170 | loss 2.7844 | top1 30.19% | top5 46.39% | 12.2 img/s\n",
      "Epoch 04 | batch 450/2170 | loss 2.7705 | top1 30.53% | top5 46.57% | 12.2 img/s\n",
      "Epoch 04 | batch 450/2170 | loss 2.7705 | top1 30.53% | top5 46.57% | 12.2 img/s\n",
      "Epoch 04 | batch 500/2170 | loss 2.7617 | top1 30.38% | top5 46.23% | 12.2 img/s\n",
      "Epoch 04 | batch 500/2170 | loss 2.7617 | top1 30.38% | top5 46.23% | 12.2 img/s\n",
      "Epoch 04 | batch 550/2170 | loss 2.7524 | top1 30.67% | top5 46.68% | 12.2 img/s\n",
      "Epoch 04 | batch 550/2170 | loss 2.7524 | top1 30.67% | top5 46.68% | 12.2 img/s\n",
      "Epoch 04 | batch 600/2170 | loss 2.7321 | top1 30.70% | top5 46.46% | 12.2 img/s\n",
      "Epoch 04 | batch 600/2170 | loss 2.7321 | top1 30.70% | top5 46.46% | 12.2 img/s\n",
      "Epoch 04 | batch 650/2170 | loss 2.7266 | top1 30.80% | top5 46.52% | 12.2 img/s\n",
      "Epoch 04 | batch 650/2170 | loss 2.7266 | top1 30.80% | top5 46.52% | 12.2 img/s\n",
      "Epoch 04 | batch 700/2170 | loss 2.7068 | top1 30.43% | top5 45.84% | 12.2 img/s\n",
      "Epoch 04 | batch 700/2170 | loss 2.7068 | top1 30.43% | top5 45.84% | 12.2 img/s\n",
      "Epoch 04 | batch 750/2170 | loss 2.7034 | top1 30.54% | top5 45.83% | 12.2 img/s\n",
      "Epoch 04 | batch 750/2170 | loss 2.7034 | top1 30.54% | top5 45.83% | 12.2 img/s\n",
      "Epoch 04 | batch 800/2170 | loss 2.6949 | top1 30.48% | top5 45.82% | 12.1 img/s\n",
      "Epoch 04 | batch 800/2170 | loss 2.6949 | top1 30.48% | top5 45.82% | 12.1 img/s\n",
      "Epoch 04 | batch 850/2170 | loss 2.6782 | top1 30.71% | top5 46.01% | 12.1 img/s\n",
      "Epoch 04 | batch 850/2170 | loss 2.6782 | top1 30.71% | top5 46.01% | 12.1 img/s\n",
      "Epoch 04 | batch 900/2170 | loss 2.6680 | top1 31.35% | top5 46.66% | 12.1 img/s\n",
      "Epoch 04 | batch 900/2170 | loss 2.6680 | top1 31.35% | top5 46.66% | 12.1 img/s\n",
      "Epoch 04 | batch 950/2170 | loss 2.6645 | top1 31.40% | top5 46.66% | 12.1 img/s\n",
      "Epoch 04 | batch 950/2170 | loss 2.6645 | top1 31.40% | top5 46.66% | 12.1 img/s\n",
      "Epoch 04 | batch 1000/2170 | loss 2.6563 | top1 31.58% | top5 46.77% | 12.1 img/s\n",
      "Epoch 04 | batch 1000/2170 | loss 2.6563 | top1 31.58% | top5 46.77% | 12.1 img/s\n",
      "Epoch 04 | batch 1050/2170 | loss 2.6491 | top1 31.48% | top5 46.60% | 12.2 img/s\n",
      "Epoch 04 | batch 1050/2170 | loss 2.6491 | top1 31.48% | top5 46.60% | 12.2 img/s\n",
      "Epoch 04 | batch 1100/2170 | loss 2.6385 | top1 31.56% | top5 46.74% | 12.2 img/s\n",
      "Epoch 04 | batch 1100/2170 | loss 2.6385 | top1 31.56% | top5 46.74% | 12.2 img/s\n",
      "Epoch 04 | batch 1150/2170 | loss 2.6297 | top1 31.83% | top5 47.05% | 12.2 img/s\n",
      "Epoch 04 | batch 1150/2170 | loss 2.6297 | top1 31.83% | top5 47.05% | 12.2 img/s\n",
      "Epoch 04 | batch 1200/2170 | loss 2.6236 | top1 31.92% | top5 47.22% | 12.2 img/s\n",
      "Epoch 04 | batch 1200/2170 | loss 2.6236 | top1 31.92% | top5 47.22% | 12.2 img/s\n",
      "Epoch 04 | batch 1250/2170 | loss 2.6144 | top1 32.05% | top5 47.38% | 12.2 img/s\n",
      "Epoch 04 | batch 1250/2170 | loss 2.6144 | top1 32.05% | top5 47.38% | 12.2 img/s\n",
      "Epoch 04 | batch 1300/2170 | loss 2.6090 | top1 31.84% | top5 47.04% | 12.2 img/s\n",
      "Epoch 04 | batch 1300/2170 | loss 2.6090 | top1 31.84% | top5 47.04% | 12.2 img/s\n",
      "Epoch 04 | batch 1350/2170 | loss 2.5997 | top1 31.68% | top5 46.81% | 12.3 img/s\n",
      "Epoch 04 | batch 1350/2170 | loss 2.5997 | top1 31.68% | top5 46.81% | 12.3 img/s\n",
      "Epoch 04 | batch 1400/2170 | loss 2.5910 | top1 31.90% | top5 46.96% | 12.3 img/s\n",
      "Epoch 04 | batch 1400/2170 | loss 2.5910 | top1 31.90% | top5 46.96% | 12.3 img/s\n",
      "Epoch 04 | batch 1450/2170 | loss 2.5817 | top1 31.89% | top5 46.85% | 12.3 img/s\n",
      "Epoch 04 | batch 1450/2170 | loss 2.5817 | top1 31.89% | top5 46.85% | 12.3 img/s\n",
      "Epoch 04 | batch 1500/2170 | loss 2.5816 | top1 32.14% | top5 47.12% | 12.3 img/s\n",
      "Epoch 04 | batch 1500/2170 | loss 2.5816 | top1 32.14% | top5 47.12% | 12.3 img/s\n",
      "Epoch 04 | batch 1550/2170 | loss 2.5798 | top1 32.23% | top5 47.11% | 12.3 img/s\n",
      "Epoch 04 | batch 1550/2170 | loss 2.5798 | top1 32.23% | top5 47.11% | 12.3 img/s\n",
      "Epoch 04 | batch 1600/2170 | loss 2.5765 | top1 32.45% | top5 47.33% | 12.3 img/s\n",
      "Epoch 04 | batch 1600/2170 | loss 2.5765 | top1 32.45% | top5 47.33% | 12.3 img/s\n",
      "Epoch 04 | batch 1650/2170 | loss 2.5701 | top1 32.67% | top5 47.50% | 12.3 img/s\n",
      "Epoch 04 | batch 1650/2170 | loss 2.5701 | top1 32.67% | top5 47.50% | 12.3 img/s\n",
      "Epoch 04 | batch 1700/2170 | loss 2.5701 | top1 32.61% | top5 47.43% | 12.3 img/s\n",
      "Epoch 04 | batch 1700/2170 | loss 2.5701 | top1 32.61% | top5 47.43% | 12.3 img/s\n",
      "Epoch 04 | batch 1750/2170 | loss 2.5714 | top1 32.44% | top5 47.25% | 12.3 img/s\n",
      "Epoch 04 | batch 1750/2170 | loss 2.5714 | top1 32.44% | top5 47.25% | 12.3 img/s\n",
      "Epoch 04 | batch 1800/2170 | loss 2.5691 | top1 32.42% | top5 47.19% | 12.3 img/s\n",
      "Epoch 04 | batch 1800/2170 | loss 2.5691 | top1 32.42% | top5 47.19% | 12.3 img/s\n",
      "Epoch 04 | batch 1850/2170 | loss 2.5607 | top1 32.56% | top5 47.23% | 12.3 img/s\n",
      "Epoch 04 | batch 1850/2170 | loss 2.5607 | top1 32.56% | top5 47.23% | 12.3 img/s\n",
      "Epoch 04 | batch 1900/2170 | loss 2.5580 | top1 32.56% | top5 47.24% | 12.4 img/s\n",
      "Epoch 04 | batch 1900/2170 | loss 2.5580 | top1 32.56% | top5 47.24% | 12.4 img/s\n",
      "Epoch 04 | batch 1950/2170 | loss 2.5556 | top1 32.43% | top5 46.98% | 12.4 img/s\n",
      "Epoch 04 | batch 1950/2170 | loss 2.5556 | top1 32.43% | top5 46.98% | 12.4 img/s\n",
      "Epoch 04 | batch 2000/2170 | loss 2.5503 | top1 32.56% | top5 47.02% | 12.4 img/s\n",
      "Epoch 04 | batch 2000/2170 | loss 2.5503 | top1 32.56% | top5 47.02% | 12.4 img/s\n",
      "Epoch 04 | batch 2050/2170 | loss 2.5482 | top1 32.53% | top5 47.02% | 12.4 img/s\n",
      "Epoch 04 | batch 2050/2170 | loss 2.5482 | top1 32.53% | top5 47.02% | 12.4 img/s\n",
      "Epoch 04 | batch 2100/2170 | loss 2.5444 | top1 32.65% | top5 47.13% | 12.4 img/s\n",
      "Epoch 04 | batch 2100/2170 | loss 2.5444 | top1 32.65% | top5 47.13% | 12.4 img/s\n",
      "Epoch 04 | batch 2150/2170 | loss 2.5402 | top1 32.64% | top5 47.09% | 12.4 img/s\n",
      "Epoch 04 | batch 2150/2170 | loss 2.5402 | top1 32.64% | top5 47.09% | 12.4 img/s\n",
      "Epoch 04 | batch 2170/2170 | loss 2.5410 | top1 32.62% | top5 47.07% | 12.4 img/s\n",
      "Epoch 04 | batch 2170/2170 | loss 2.5410 | top1 32.62% | top5 47.07% | 12.4 img/s\n",
      "\n",
      "Epoch 04/50 | 3172.5s\n",
      "  Train(mixup): loss 2.5410 | top1 32.62% | top5 47.07%\n",
      "  Train(clean): loss 1.5340 | top1 82.65% | top5 96.11%\n",
      "  Val:          loss 1.6826 | top1 77.60% | top5 94.19%\n",
      "\n",
      "Epoch 04/50 | 3172.5s\n",
      "  Train(mixup): loss 2.5410 | top1 32.62% | top5 47.07%\n",
      "  Train(clean): loss 1.5340 | top1 82.65% | top5 96.11%\n",
      "  Val:          loss 1.6826 | top1 77.60% | top5 94.19%\n",
      "  ✓ Saved new best: d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\\best_efficientnet_b3.pth (val Top-1=77.60%)\n",
      "  ✓ Saved new best: d:\\VSC FILES\\testtrain\\runs\\efficientnet_b3_optimized-20251021-100525\\best_efficientnet_b3.pth (val Top-1=77.60%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=remaining)\n\u001b[32m     16\u001b[39m t0 = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m tr = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m tr_clean = evaluate(train_eval_loader)\n\u001b[32m     19\u001b[39m va = evaluate(val_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch)\u001b[39m\n\u001b[32m     15\u001b[39m     y_b = y_b.to(dml)\n\u001b[32m     16\u001b[39m     outputs = model(images)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     loss = \u001b[43mmixup_criterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     19\u001b[39m     outputs = model(images)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mmixup_criterion\u001b[39m\u001b[34m(criterion, pred, y_a, y_b, lam)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmixup_criterion\u001b[39m(criterion, pred, y_a, y_b, lam):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lam * \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m)\u001b[49m + (\u001b[32m1\u001b[39m - lam) * criterion(pred, y_b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JP\\anaconda3\\envs\\dml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JP\\anaconda3\\envs\\dml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JP\\anaconda3\\envs\\dml\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JP\\anaconda3\\envs\\dml\\Lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3103\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "best_acc = 0.0\n",
    "best_path = os.path.join(run_dir, f\"best_{model_name}.pth\")\n",
    "no_improve_count = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Unfreeze after warmup\n",
    "    if epoch == head_warmup_epochs + 1:\n",
    "        print(f\"\\n=== Unfreezing backbone at epoch {epoch} ===\")\n",
    "        for name, p in model.named_parameters():\n",
    "            p.requires_grad = True\n",
    "        optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr/10, weight_decay=weight_decay)\n",
    "        remaining = max(1, epochs - epoch + 1)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=remaining)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    tr = train_one_epoch(epoch)\n",
    "    tr_clean = evaluate(train_eval_loader)\n",
    "    va = evaluate(val_loader)\n",
    "    scheduler.step()\n",
    "    dt = time.perf_counter() - t0\n",
    "    \n",
    "    row = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": round(tr[\"loss\"], 6),\n",
    "        \"train_top1\": round(tr[\"top1\"], 4),\n",
    "        \"train_top5\": round(tr[\"top5\"], 4),\n",
    "        \"train_eval_loss\": round(tr_clean[\"loss\"], 6),\n",
    "        \"train_eval_top1\": round(tr_clean[\"top1\"], 4),\n",
    "        \"train_eval_top5\": round(tr_clean[\"top5\"], 4),\n",
    "        \"val_loss\": round(va[\"loss\"], 6),\n",
    "        \"val_top1\": round(va[\"top1\"], 4),\n",
    "        \"val_top5\": round(va[\"top5\"], 4),\n",
    "        \"time_s\": round(dt, 3)\n",
    "    }\n",
    "    history.append(row)\n",
    "    log_epoch(row)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch:02d}/{epochs} | {dt:.1f}s\")\n",
    "    print(f\"  Train(mixup): loss {tr['loss']:.4f} | top1 {tr['top1']:.2f}% | top5 {tr['top5']:.2f}%\")\n",
    "    print(f\"  Train(clean): loss {tr_clean['loss']:.4f} | top1 {tr_clean['top1']:.2f}% | top5 {tr_clean['top5']:.2f}%\")\n",
    "    print(f\"  Val:          loss {va['loss']:.4f} | top1 {va['top1']:.2f}% | top5 {va['top5']:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if va[\"top1\"] > best_acc:\n",
    "        best_acc = va[\"top1\"]\n",
    "        no_improve_count = 0\n",
    "        torch.save({\n",
    "            \"model_name\": model_name,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"classes\": train_ds.classes,\n",
    "            \"image_size\": image_size,\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"epoch\": epoch,\n",
    "            \"val_top1\": va[\"top1\"],\n",
    "            \"val_top5\": va[\"top5\"]\n",
    "        }, best_path)\n",
    "        print(f\"  ✓ Saved new best: {best_path} (val Top-1={best_acc:.2f}%)\")\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"\\n[Early Stop] No improvement for {patience} epochs. Stopping.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n=== Training Complete ===\")\n",
    "print(f\"Best val Top-1: {best_acc:.2f}%\")\n",
    "print(f\"Best model saved to: {best_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af618ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "epochs_ = [h[\"epoch\"] for h in history]\n",
    "tr_loss = [h[\"train_loss\"] for h in history]\n",
    "va_loss = [h[\"val_loss\"] for h in history]\n",
    "tr_eval_loss = [h[\"train_eval_loss\"] for h in history]\n",
    "tr_t1 = [h[\"train_top1\"] for h in history]\n",
    "va_t1 = [h[\"val_top1\"] for h in history]\n",
    "tr_eval_t1 = [h[\"train_eval_top1\"] for h in history]\n",
    "tr_t5 = [h[\"train_top5\"] for h in history]\n",
    "va_t5 = [h[\"val_top5\"] for h in history]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_, tr_loss, '-o', label='train (mixup)', alpha=0.7)\n",
    "axes[0].plot(epochs_, tr_eval_loss, '-s', label='train (clean)', alpha=0.7)\n",
    "axes[0].plot(epochs_, va_loss, '-^', label='val', linewidth=2)\n",
    "axes[0].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Top-1 Accuracy\n",
    "axes[1].plot(epochs_, tr_t1, '-o', label='train (mixup)', alpha=0.7)\n",
    "axes[1].plot(epochs_, tr_eval_t1, '-s', label='train (clean)', alpha=0.7)\n",
    "axes[1].plot(epochs_, va_t1, '-^', label='val', linewidth=2)\n",
    "axes[1].set_title('Top-1 Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# Top-5 Accuracy\n",
    "axes[2].plot(epochs_, tr_t5, '-o', label='train', alpha=0.7)\n",
    "axes[2].plot(epochs_, va_t5, '-^', label='val', linewidth=2)\n",
    "axes[2].set_title('Top-5 Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy (%)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(run_dir, \"training_curves.png\"), dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved training curves to: {os.path.join(run_dir, 'training_curves.png')}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5750df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on all splits\n",
    "print(\"\\n=== Loading Best Model ===\")\n",
    "ckpt = torch.load(best_path, map_location=\"cpu\")\n",
    "classes = ckpt[\"classes\"]\n",
    "\n",
    "test_model = models.efficientnet_b3(weights=None)\n",
    "test_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=dropout_p),\n",
    "    nn.Linear(test_model.classifier[1].in_features, len(classes))\n",
    ")\n",
    "test_model.load_state_dict(ckpt[\"model_state\"])\n",
    "test_model = test_model.to(dml).eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {ckpt['epoch']}\")\n",
    "print(f\"Best val Top-1: {ckpt['val_top1']:.2f}% | Top-5: {ckpt['val_top5']:.2f}%\")\n",
    "\n",
    "def evaluate_detailed(model, loader, split_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(dml)\n",
    "            labels = labels.to(dml)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            all_probs.append(probs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_probs = torch.cat(all_probs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    # Top-1 predictions\n",
    "    top1_probs, top1_preds = all_probs.max(dim=1)\n",
    "    top1_correct = (top1_preds == all_labels).float()\n",
    "    top1_acc = top1_correct.mean().item() * 100\n",
    "    \n",
    "    # Top-5 predictions\n",
    "    top5_probs, top5_preds = all_probs.topk(5, dim=1)\n",
    "    top5_correct = top5_preds.eq(all_labels.view(-1, 1).expand_as(top5_preds))\n",
    "    top5_acc = top5_correct.any(dim=1).float().mean().item() * 100\n",
    "    \n",
    "    # Count samples with top1 confidence >= 80%\n",
    "    top1_high_conf = (top1_probs >= 0.8).sum().item()\n",
    "    top1_high_conf_correct = ((top1_probs >= 0.8) & (top1_preds == all_labels)).sum().item()\n",
    "    \n",
    "    # Count samples where correct class is in top5 with >= 50% confidence\n",
    "    correct_in_top5_mask = top5_correct.any(dim=1)\n",
    "    correct_class_probs = all_probs[torch.arange(len(all_labels)), all_labels]\n",
    "    top5_high_conf = (correct_in_top5_mask & (correct_class_probs >= 0.5)).sum().item()\n",
    "    \n",
    "    # Calculate additional metrics: accuracy, F1, precision, recall\n",
    "    y_true = all_labels.numpy()\n",
    "    y_pred = top1_preds.numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0) * 100\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0) * 100\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0) * 100\n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0) * 100\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0) * 100\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0) * 100\n",
    "    \n",
    "    print(f\"\\n{split_name.upper()} Results:\")\n",
    "    print(f\"  Total samples: {len(all_labels)}\")\n",
    "    print(f\"  Top-1 Accuracy: {top1_acc:.2f}%\")\n",
    "    print(f\"  Top-5 Accuracy: {top5_acc:.2f}%\")\n",
    "    print(f\"  Accuracy (sklearn): {accuracy:.2f}%\")\n",
    "    print(f\"  F1 Score (macro): {f1_macro:.2f}%\")\n",
    "    print(f\"  F1 Score (weighted): {f1_weighted:.2f}%\")\n",
    "    print(f\"  Precision (macro): {precision_macro:.2f}%\")\n",
    "    print(f\"  Precision (weighted): {precision_weighted:.2f}%\")\n",
    "    print(f\"  Recall (macro): {recall_macro:.2f}%\")\n",
    "    print(f\"  Recall (weighted): {recall_weighted:.2f}%\")\n",
    "    print(f\"  Top-1 with ≥80% confidence: {top1_high_conf} ({top1_high_conf/len(all_labels)*100:.2f}%)\")\n",
    "    print(f\"    └─ Correct among high-conf: {top1_high_conf_correct} ({top1_high_conf_correct/max(1,top1_high_conf)*100:.2f}%)\")\n",
    "    print(f\"  Correct class in Top-5 with ≥50% confidence: {top5_high_conf} ({top5_high_conf/len(all_labels)*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        \"split\": split_name,\n",
    "        \"total\": len(all_labels),\n",
    "        \"top1_acc\": top1_acc,\n",
    "        \"top5_acc\": top5_acc,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"recall_weighted\": recall_weighted,\n",
    "        \"top1_conf80_count\": top1_high_conf,\n",
    "        \"top1_conf80_correct\": top1_high_conf_correct,\n",
    "        \"top5_conf50_count\": top5_high_conf,\n",
    "        \"all_probs\": all_probs,\n",
    "        \"all_labels\": all_labels,\n",
    "        \"all_preds\": top1_preds\n",
    "    }\n",
    "\n",
    "train_results = evaluate_detailed(test_model, train_eval_loader, \"train\")\n",
    "val_results = evaluate_detailed(test_model, val_loader, \"val\")\n",
    "test_results = evaluate_detailed(test_model, test_loader, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e448d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy analysis\n",
    "def per_class_accuracy(all_probs, all_labels, classes):\n",
    "    num_classes = len(classes)\n",
    "    class_correct = torch.zeros(num_classes)\n",
    "    class_total = torch.zeros(num_classes)\n",
    "    \n",
    "    preds = all_probs.argmax(dim=1)\n",
    "    for label, pred in zip(all_labels, preds):\n",
    "        class_total[label] += 1\n",
    "        if label == pred:\n",
    "            class_correct[label] += 1\n",
    "    \n",
    "    class_acc = (class_correct / class_total.clamp(min=1)) * 100\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    sorted_indices = torch.argsort(class_acc, descending=True)\n",
    "    \n",
    "    return class_acc, class_total, sorted_indices\n",
    "\n",
    "print(\"\\n=== Per-Class Accuracy (Test Set) ===\")\n",
    "class_acc, class_total, sorted_idx = per_class_accuracy(\n",
    "    test_results[\"all_probs\"],\n",
    "    test_results[\"all_labels\"],\n",
    "    classes\n",
    ")\n",
    "\n",
    "# Top 10 best classes\n",
    "print(\"\\nTop 10 Best Classes:\")\n",
    "for i in range(min(10, len(classes))):\n",
    "    idx = sorted_idx[i]\n",
    "    print(f\"  {i+1}. {classes[idx]}: {class_acc[idx]:.2f}% ({int(class_total[idx])} samples)\")\n",
    "\n",
    "# Bottom 10 worst classes\n",
    "print(\"\\nBottom 10 Worst Classes:\")\n",
    "for i in range(min(10, len(classes))):\n",
    "    idx = sorted_idx[-(i+1)]\n",
    "    print(f\"  {i+1}. {classes[idx]}: {class_acc[idx]:.2f}% ({int(class_total[idx])} samples)\")\n",
    "\n",
    "# Save per-class results\n",
    "per_class_results = []\n",
    "for i in range(len(classes)):\n",
    "    per_class_results.append({\n",
    "        \"class\": classes[i],\n",
    "        \"accuracy\": float(class_acc[i]),\n",
    "        \"total\": int(class_total[i])\n",
    "    })\n",
    "\n",
    "with open(os.path.join(run_dir, \"per_class_accuracy.json\"), \"w\") as f:\n",
    "    json.dump(per_class_results, f, indent=2)\n",
    "print(f\"\\nSaved per-class accuracy to: {os.path.join(run_dir, 'per_class_accuracy.json')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class accuracy\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "class_names_sorted = [classes[i] for i in sorted_idx]\n",
    "acc_sorted = [class_acc[i].item() for i in sorted_idx]\n",
    "\n",
    "bars = ax.bar(range(len(classes)), acc_sorted, color='steelblue', alpha=0.7)\n",
    "\n",
    "# Color code: green for >80%, yellow for 50-80%, red for <50%\n",
    "for i, (bar, acc) in enumerate(zip(bars, acc_sorted)):\n",
    "    if acc >= 80:\n",
    "        bar.set_color('green')\n",
    "    elif acc >= 50:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "ax.set_xlabel('Class (sorted by accuracy)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Per-Class Accuracy (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=80, color='green', linestyle='--', alpha=0.5, label='80% threshold')\n",
    "ax.axhline(y=50, color='orange', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(run_dir, \"per_class_accuracy.png\"), dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved per-class accuracy chart to: {os.path.join(run_dir, 'per_class_accuracy.png')}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40707e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for top confusing classes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "preds = test_results[\"all_probs\"].argmax(dim=1).numpy()\n",
    "labels = test_results[\"all_labels\"].numpy()\n",
    "\n",
    "# Full confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "# Find most confused pairs\n",
    "confused_pairs = []\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confused_pairs.append((i, j, cm[i, j]))\n",
    "\n",
    "confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\n=== Top 10 Most Confused Class Pairs ===\")\n",
    "for i, (true_idx, pred_idx, count) in enumerate(confused_pairs[:10], 1):\n",
    "    print(f\"  {i}. {classes[true_idx]} → {classes[pred_idx]}: {count} times\")\n",
    "\n",
    "# Plot confusion matrix for top 20 classes\n",
    "top_n = 20\n",
    "top_classes_idx = sorted_idx[:top_n].tolist()\n",
    "cm_subset = cm[np.ix_(top_classes_idx, top_classes_idx)]\n",
    "class_names_subset = [classes[i] for i in top_classes_idx]\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(cm_subset, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_subset, yticklabels=class_names_subset,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.title(f'Confusion Matrix - Top {top_n} Classes (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(run_dir, \"confusion_matrix.png\"), dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nSaved confusion matrix to: {os.path.join(run_dir, 'confusion_matrix.png')}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n=== Classification Report (Test Set) ===\")\n",
    "y_true = test_results[\"all_labels\"].numpy()\n",
    "y_pred = test_results[\"all_preds\"].numpy()\n",
    "\n",
    "# Full classification report\n",
    "report = classification_report(y_true, y_pred, target_names=classes, digits=4, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# Save classification report\n",
    "report_path = os.path.join(run_dir, \"classification_report.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"Classification Report (Test Set)\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(report)\n",
    "print(f\"\\nSaved classification report to: {report_path}\")\n",
    "\n",
    "# Generate per-class metrics\n",
    "report_dict = classification_report(y_true, y_pred, target_names=classes, output_dict=True, zero_division=0)\n",
    "per_class_metrics = []\n",
    "for class_name in classes:\n",
    "    if class_name in report_dict:\n",
    "        metrics = report_dict[class_name]\n",
    "        per_class_metrics.append({\n",
    "            \"class\": class_name,\n",
    "            \"precision\": metrics[\"precision\"] * 100,\n",
    "            \"recall\": metrics[\"recall\"] * 100,\n",
    "            \"f1_score\": metrics[\"f1-score\"] * 100,\n",
    "            \"support\": metrics[\"support\"]\n",
    "        })\n",
    "\n",
    "# Save per-class metrics\n",
    "metrics_path = os.path.join(run_dir, \"per_class_metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(per_class_metrics, f, indent=2)\n",
    "print(f\"Saved per-class metrics to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "print(\"\\n=== Exporting to ONNX ===\")\n",
    "\n",
    "onnx_path = os.path.join(run_dir, \"model.onnx\")\n",
    "dummy_input = torch.randn(1, 3, image_size, image_size).to(dml)\n",
    "\n",
    "test_model.eval()\n",
    "torch.onnx.export(\n",
    "    test_model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"✓ Exported ONNX model to: {onnx_path}\")\n",
    "print(f\"  Input shape: [batch_size, 3, {image_size}, {image_size}]\")\n",
    "print(f\"  Output shape: [batch_size, {num_classes}]\")\n",
    "print(f\"  Class names saved to: {os.path.join(run_dir, 'class_names.json')}\")\n",
    "\n",
    "# Test ONNX model\n",
    "import onnx\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"✓ ONNX model is valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary report\n",
    "summary = {\n",
    "    \"model\": model_name,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"image_size\": image_size,\n",
    "    \"total_epochs_trained\": len(history),\n",
    "    \"best_epoch\": ckpt[\"epoch\"],\n",
    "    \"best_val_top1\": ckpt[\"val_top1\"],\n",
    "    \"best_val_top5\": ckpt[\"val_top5\"],\n",
    "    \"train_results\": {\n",
    "        \"total\": train_results[\"total\"],\n",
    "        \"top1_acc\": train_results[\"top1_acc\"],\n",
    "        \"top5_acc\": train_results[\"top5_acc\"],\n",
    "        \"accuracy\": train_results[\"accuracy\"],\n",
    "        \"f1_macro\": train_results[\"f1_macro\"],\n",
    "        \"f1_weighted\": train_results[\"f1_weighted\"],\n",
    "        \"precision_macro\": train_results[\"precision_macro\"],\n",
    "        \"precision_weighted\": train_results[\"precision_weighted\"],\n",
    "        \"recall_macro\": train_results[\"recall_macro\"],\n",
    "        \"recall_weighted\": train_results[\"recall_weighted\"],\n",
    "        \"top1_conf80_count\": train_results[\"top1_conf80_count\"],\n",
    "        \"top1_conf80_percent\": train_results[\"top1_conf80_count\"] / train_results[\"total\"] * 100,\n",
    "        \"top5_conf50_count\": train_results[\"top5_conf50_count\"],\n",
    "        \"top5_conf50_percent\": train_results[\"top5_conf50_count\"] / train_results[\"total\"] * 100\n",
    "    },\n",
    "    \"val_results\": {\n",
    "        \"total\": val_results[\"total\"],\n",
    "        \"top1_acc\": val_results[\"top1_acc\"],\n",
    "        \"top5_acc\": val_results[\"top5_acc\"],\n",
    "        \"accuracy\": val_results[\"accuracy\"],\n",
    "        \"f1_macro\": val_results[\"f1_macro\"],\n",
    "        \"f1_weighted\": val_results[\"f1_weighted\"],\n",
    "        \"precision_macro\": val_results[\"precision_macro\"],\n",
    "        \"precision_weighted\": val_results[\"precision_weighted\"],\n",
    "        \"recall_macro\": val_results[\"recall_macro\"],\n",
    "        \"recall_weighted\": val_results[\"recall_weighted\"],\n",
    "        \"top1_conf80_count\": val_results[\"top1_conf80_count\"],\n",
    "        \"top1_conf80_percent\": val_results[\"top1_conf80_count\"] / val_results[\"total\"] * 100,\n",
    "        \"top5_conf50_count\": val_results[\"top5_conf50_count\"],\n",
    "        \"top5_conf50_percent\": val_results[\"top5_conf50_count\"] / val_results[\"total\"] * 100\n",
    "    },\n",
    "    \"test_results\": {\n",
    "        \"total\": test_results[\"total\"],\n",
    "        \"top1_acc\": test_results[\"top1_acc\"],\n",
    "        \"top5_acc\": test_results[\"top5_acc\"],\n",
    "        \"accuracy\": test_results[\"accuracy\"],\n",
    "        \"f1_macro\": test_results[\"f1_macro\"],\n",
    "        \"f1_weighted\": test_results[\"f1_weighted\"],\n",
    "        \"precision_macro\": test_results[\"precision_macro\"],\n",
    "        \"precision_weighted\": test_results[\"precision_weighted\"],\n",
    "        \"recall_macro\": test_results[\"recall_macro\"],\n",
    "        \"recall_weighted\": test_results[\"recall_weighted\"],\n",
    "        \"top1_conf80_count\": test_results[\"top1_conf80_count\"],\n",
    "        \"top1_conf80_percent\": test_results[\"top1_conf80_count\"] / test_results[\"total\"] * 100,\n",
    "        \"top5_conf50_count\": test_results[\"top5_conf50_count\"],\n",
    "        \"top5_conf50_percent\": test_results[\"top5_conf50_count\"] / test_results[\"total\"] * 100\n",
    "    },\n",
    "    \"files\": {\n",
    "        \"best_model\": best_path,\n",
    "        \"onnx_model\": onnx_path,\n",
    "        \"class_names\": os.path.join(run_dir, \"class_names.json\"),\n",
    "        \"training_curves\": os.path.join(run_dir, \"training_curves.png\"),\n",
    "        \"per_class_accuracy\": os.path.join(run_dir, \"per_class_accuracy.json\"),\n",
    "        \"per_class_metrics\": os.path.join(run_dir, \"per_class_metrics.json\"),\n",
    "        \"classification_report\": os.path.join(run_dir, \"classification_report.txt\"),\n",
    "        \"confusion_matrix\": os.path.join(run_dir, \"confusion_matrix.png\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(run_dir, \"summary.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Classes: {num_classes}\")\n",
    "print(f\"Image size: {image_size}x{image_size}\")\n",
    "print(f\"Epochs trained: {len(history)}\")\n",
    "print(f\"Best epoch: {ckpt['epoch']}\")\n",
    "print(f\"\\nBest Validation Accuracy:\")\n",
    "print(f\"  Top-1: {ckpt['val_top1']:.2f}%\")\n",
    "print(f\"  Top-5: {ckpt['val_top5']:.2f}%\")\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Top-1 Accuracy: {test_results['top1_acc']:.2f}%\")\n",
    "print(f\"  Top-5 Accuracy: {test_results['top5_acc']:.2f}%\")\n",
    "print(f\"  Accuracy: {test_results['accuracy']:.2f}%\")\n",
    "print(f\"  F1 Score (macro): {test_results['f1_macro']:.2f}%\")\n",
    "print(f\"  F1 Score (weighted): {test_results['f1_weighted']:.2f}%\")\n",
    "print(f\"  Precision (macro): {test_results['precision_macro']:.2f}%\")\n",
    "print(f\"  Precision (weighted): {test_results['precision_weighted']:.2f}%\")\n",
    "print(f\"  Recall (macro): {test_results['recall_macro']:.2f}%\")\n",
    "print(f\"  Recall (weighted): {test_results['recall_weighted']:.2f}%\")\n",
    "print(f\"  Top-1 predictions with ≥80% confidence: {test_results['top1_conf80_count']} ({test_results['top1_conf80_count']/test_results['total']*100:.2f}%)\")\n",
    "print(f\"  Correct class in Top-5 with ≥50% confidence: {test_results['top5_conf50_count']} ({test_results['top5_conf50_count']/test_results['total']*100:.2f}%)\")\n",
    "print(f\"\\nOutput files saved to: {run_dir}\")\n",
    "print(f\"  - Best model (PyTorch): {os.path.basename(best_path)}\")\n",
    "print(f\"  - ONNX model: model.onnx\")\n",
    "print(f\"  - Class names: class_names.json\")\n",
    "print(f\"  - Per-class metrics: per_class_metrics.json\")\n",
    "print(f\"  - Classification report: classification_report.txt\")\n",
    "print(f\"  - Summary: summary.json\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
