# EfficientNet-B3 Food Classification Model Training

**Project**: NutriSight Food Recognition System  
**Model**: EfficientNet-B3 with Transfer Learning  
**Task**: Multi-class Image Classification (124 Food Categories)  
**Date**: October 2024  
**Framework**: PyTorch + DirectML (AMD/Intel GPU Support)

---

## üìä Executive Summary

This document describes the training methodology and results for a deep learning model capable of recognizing **124 different food items** from images with **86.87% accuracy** (Test Top-1).

### Key Results

- **Test Accuracy (Top-1)**: 86.87%
- **Test Top-5 Accuracy**: 97.30% (correct answer in top 5 predictions)
- **High-Confidence Predictions (Test, ‚â•80%)**: 3581 / 4340 (82.51% of test samples); accuracy among these high-confidence predictions: 95.14%
- **Epochs Trained**: 17 (best model saved at epoch 17)
- **Model Size**: 45MB (ONNX format for deployment)

---

## üéØ Problem Statement

**Goal**: Build an accurate food recognition system for nutritional tracking applications.

**Challenge**:

- Recognize 124 different food categories from photos
- Handle visual similarities between foods (e.g., different types of cakes, pasta dishes)
- Achieve high accuracy while maintaining reasonable inference speed
- Deploy on resource-constrained environments (web servers)

---

## üìÅ Dataset

### Dataset Structure

```
Total Images: 43,400
Food Categories: 124 classes
‚îú‚îÄ‚îÄ Examples: Pizza, Hamburger, Sushi, Tacos, Ice Cream, etc.
‚îú‚îÄ‚îÄ Images per class: ~350 images (balanced distribution)
‚îî‚îÄ‚îÄ Image Resolution: 252√ó252 pixels
```

### Data Source

- **Base Dataset**: Selected categories from Food-101 (not all 101 classes are included)
- **Extended Dataset**: Additional images collected from our custom/farmed datasets (locally sourced)
- **Note**: The final 124-class dataset is a mix of selected Food-101 categories and our own farmed/custom images ‚Äî some Food-101 classes were omitted and replaced/augmented by custom data.
- **Split Method**: Stratified random split to ensure balanced class distribution

---

## üß† Model Architecture

### Base Model: EfficientNet-B3

**Why EfficientNet-B3?**

1. **Efficient Design**: Balances accuracy and computational cost
2. **Compound Scaling**: Uniformly scales network depth, width, and resolution
3. **Pre-trained Weights**: Leverages ImageNet knowledge (transfer learning)
4. **Mobile-Friendly**: Suitable for deployment on resource-constrained devices

**Architecture Overview**:s

```
Input Image (252√ó252√ó3)
    ‚Üì
EfficientNet-B3 Backbone (12M parameters)
‚îú‚îÄ‚îÄ Compound scaled CNN layers
‚îú‚îÄ‚îÄ Mobile Inverted Bottleneck Convolution (MBConv)
‚îú‚îÄ‚îÄ Squeeze-and-Excitation blocks
‚îî‚îÄ‚îÄ Feature Extraction
    ‚Üì
Custom Classification Head
‚îú‚îÄ‚îÄ Dropout Layer (30% dropout rate)
‚îî‚îÄ‚îÄ Fully Connected Layer (‚Üí 124 classes)
    ‚Üì
Output: Class Probabilities (124 values)
```

**Model Parameters**:

- Total Parameters: **12.0M**
- Trainable Parameters: **12.0M** (after warmup phase)
- Model Size: **~45MB** (ONNX format)

---

## üî¨ Training Methodology

### 1. Transfer Learning Strategy

**Two-Phase Training Approach**:

**Phase 1: Head Warmup (Epochs 1-3)**

- Freeze backbone (EfficientNet-B3 pre-trained layers)
- Train only classification head
- Purpose: Adapt final layers to food recognition task
- Learning Rate: 1√ó10‚Åª¬≥

**Phase 2: Fine-tuning (Epochs 4-17)**

- Unfreeze entire network
- Train all layers with lower learning rate
- Purpose: Fine-tune feature extractors for food-specific patterns
- Learning Rate: 1√ó10‚Åª‚Å¥ (10√ó reduction)

**Why This Approach?**

- Prevents catastrophic forgetting of pre-trained features
- Faster convergence compared to training from scratch
- Better generalization on limited dataset

---

### 2. Data Augmentation Techniques

**Purpose**: Artificially increase dataset diversity to improve generalization.

**Training Augmentations Applied**:

| Augmentation             | Parameters                                            | Purpose                              |
| ------------------------ | ----------------------------------------------------- | ------------------------------------ |
| **RandomResizedCrop**    | scale=(0.7, 1.0)                                      | Simulate different camera distances  |
| **RandomHorizontalFlip** | p=0.5                                                 | Handle left-right symmetry           |
| **RandomRotation**       | ¬±20¬∞                                                  | Account for camera tilt              |
| **ColorJitter**          | brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1 | Simulate lighting variations         |
| **RandomAffine**         | translate=(0.1, 0.1)                                  | Handle slight camera shifts          |
| **RandomErasing**        | p=0.2, scale=(0.02, 0.15)                             | Simulate occlusions (e.g., utensils) |

**Validation/Test Preprocessing**:

- Resize to 284√ó284 pixels
- Center crop to 252√ó252 pixels
- Normalize with ImageNet statistics

---

### 3. Regularization Techniques

**Purpose**: Prevent overfitting and improve model generalization.

#### A. Mixup Data Augmentation

- **Algorithm**: Blend two training images and their labels
- **Formula**:
  ```
  mixed_image = Œª √ó image_A + (1-Œª) √ó image_B
  mixed_label = Œª √ó label_A + (1-Œª) √ó label_B
  where Œª ~ Beta(0.2, 0.2)
  ```
- **Effect**: Forces model to learn more robust features
- **Parameter**: Œ± = 0.2

#### B. Label Smoothing

- **Purpose**: Prevent overconfident predictions
- **Formula**:
  ```
  smoothed_label = (1 - Œµ) √ó one_hot_label + Œµ/num_classes
  where Œµ = 0.1
  ```
- **Effect**: Improves calibration and generalization

#### C. Dropout

- **Rate**: 30% (p=0.3)
- **Location**: Before final classification layer
- **Effect**: Prevents co-adaptation of neurons

#### D. Weight Decay (L2 Regularization)

- **Rate**: 1√ó10‚Åª‚Å¥
- **Effect**: Penalizes large weights, promotes simpler models

---

### 4. Training Configuration

| Hyperparameter                | Value / Defaults                                                                 | Notes & Rationale                                                                                      |
| ----------------------------- | -------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| **Optimizer**                 | SGD (baseline): momentum=0.9, nesterov=True, weight_decay=1e-4                   | Baseline training used SGD with momentum for stability and to avoid DirectML CPU-fallbacks with AdamW. |
|                               | AdamW (experiments): betas=(0.9,0.999), weight_decay=1e-2                        | AdamW used in some optimized notebooks for faster convergence; can trigger CPU fallbacks on DirectML.  |
| **Learning rate (head‚Üífine)** | Head warmup: 1e-3 (epochs 1‚Äì3) ‚Üí Fine-tune: 1e-4 (epochs 4+)                     | Short head warmup lets classifier adapt; lower LR when unfreezing prevents large weight updates.       |
| **LR scheduler**              | CosineAnnealingLR (T_max = effective training epochs) + linear warmup (3 epochs) | Smooth decay after warmup; min_lr typically set ‚âà 1e-6.                                                |
| **Batch size**                | 16                                                                               | Balance GPU memory and gradient stability.                                                             |
| **Epochs / early stop**       | max 50 (patience=15) ‚Äî baseline stopped at epoch 17                              | Early stopping on validation Top-1 to prevent overfitting.                                             |
| **Loss**                      | Cross-Entropy with label smoothing Œµ=0.1                                         | Label smoothing improves calibration and reduces overconfidence.                                       |
| **Regularization**            | Dropout p=0.3; Mixup Œ±=0.2; Weight decay per-optimizer (SGD 1e-4 / AdamW 1e-2)   | Mixup + dropout help generalization across visually-similar classes.                                   |
| **Input size**                | 252√ó252 px                                                                       | Matches EfficientNet-B3 resolution used for pretrained weights.                                        |

Notes:

- Warmup: we first train the classification head for a few epochs (default 3) at the higher learning rate to adapt the head, then unfreeze and continue fine-tuning with a lower LR and Cosine annealing.
- Weight-decay: the baseline SGD runs use 1e-4; AdamW experiments use a larger weight-decay (e.g., 1e-2) because Adam-style optimizers interact differently with weight decay.
- DirectML note: if you run training on DirectML and observe warnings about operators (e.g., aten::lerp) falling back to CPU when using AdamW, prefer SGD for full-GPU throughput.
- These hyperparameters were chosen to balance stable convergence, robust generalization (mixup/label-smoothing), and reproducibility across experiments.

---

### 5. Early Stopping

**Purpose**: Automatically stop training when model stops improving.

**Configuration**:

- **Patience**: 15 epochs
- **Metric**: Validation Top-1 Accuracy
- **Result**: Training stopped at epoch 17 (best: epoch 17)

**Why It Matters**:

- Prevents overfitting to training data
- Saves computational resources
- Ensures best model is used for deployment

---

## üìà Training Results

### Learning Curves

#### Accuracy Over Epochs

```
Phase 1 (Head Warmup - Epochs 1-3):
‚îú‚îÄ‚îÄ Validation accuracy improved quickly during head warmup.
‚îî‚îÄ‚îÄ Fast initial learning

Phase 2 (Full Fine-tuning - Epochs 4-17):
‚îú‚îÄ‚îÄ Strong improvements after unfreezing; training stopped early when validation Top-1 plateaued.
‚îî‚îÄ‚îÄ Best validation Top-1 accuracy: 85.90% (epoch 17)
```

#### Training Speed

```
Warmup Phase (Epochs 1-3):
‚îú‚îÄ‚îÄ Speed: ~51 images/second
‚îî‚îÄ‚îÄ Time per epoch: ~1,000 seconds (~17 minutes)

Fine-tuning Phase (Epochs 4-24):
‚îú‚îÄ‚îÄ Speed: ~9.5 images/second
‚îî‚îÄ‚îÄ Time per epoch: ~4,200 seconds (~70 minutes)

Reason for slowdown: Full model backpropagation (12M params)
```

---

## üéØ Final Model Performance

### Test Set Results (4,340 images)

| Metric                | Value  | Interpretation                          |
| --------------------- | ------ | --------------------------------------- |
| **Top-1 Accuracy**    | 86.87% | Correct on first guess 86.87% of time   |
| **Top-5 Accuracy**    | 97.30% | Correct answer in top 5: 97.30% of time |
| **Precision (macro)** | 87.09% | Macro precision on test set             |
| **Recall (macro)**    | 86.87% | Macro recall on test set                |
| **F1 Score (macro)**  | 86.76% | Balanced precision/recall               |

### Confidence Analysis

**High-Confidence Predictions (Test, ‚â•80% confidence):**

- **Count / Percentage**: 3581 / 4340 (82.51% of test predictions)
- **Accuracy among high-confidence predictions**: 95.14%
- **Use Case**: These predictions can be treated as high-trust; consider human review for the remainder.

**Medium-Confidence Predictions** (50-80% confidence):

- **Percentage**: ~42% of predictions
- **Accuracy**: ~85-90% (good but review recommended)

**Low-Confidence Predictions** (<50% confidence):

- **Percentage**: ~24% of predictions
- **Recommendation**: Flag for human review in production

---

## üìä Per-Class Performance

### Best Performing Classes (100% accuracy)

Examples of foods the model recognizes perfectly:

- Apple: 100%
- Balut: 100%
- Frozen Yogurt: 100%
- Leche Flan: 100%
- Sunny Side Up: 100%
- Several Filipino dishes: Baked Tahong, Chicken Tinola, Daing na Bangus, Isaw Manok, Pritong Galunggong

**Why?** These foods have distinctive visual features, consistent appearance, and minimal intra-class variation.

### Challenging Classes (54-69% accuracy)

Foods with more variability or visual similarity:

- Chocolate Mousse: 54.3%
- Grilled Cheese Sandwich: 54.3%
- Omelette: 60.0%
- Pork Bistek: 60.0%
- Chocolate Cake: 62.9%
- Pork Chop: 65.7%
- Steak: 65.7%
- Tiramisu: 65.7%

**Why?** High visual similarity, regional variations, presentation differences, or overlapping ingredients with other classes.

---

## üîÑ Comparison: Training vs Validation vs Test

| Metric         | Train  | Validation | Test   |
| -------------- | ------ | ---------- | ------ |
| Top-1 Accuracy | 99.66% | 85.90%     | 86.87% |
| Top-5 Accuracy | 99.99% | 96.94%     | 97.30% |
| F1 Score       | 99.66% | 85.84%     | 86.76% |

**Observations**:

- **Train >> Val/Test**: Expected behavior (model sees training data during learning)
- **Val ‚âà Test**: Excellent generalization (no overfitting!)
- **Gap (~12%)**: Reasonable for 124-class problem with regularization

---

## üß™ Technical Algorithms Used

### 1. **Convolutional Neural Networks (CNNs)**

- **Purpose**: Extract visual features from images
- **Components**: Convolution layers, pooling layers, activation functions
- **Why**: Effective for spatial pattern recognition in images

### 2. **Transfer Learning**

- **Concept**: Use knowledge from ImageNet (1000 classes) for food recognition
- **Benefit**: Reduces training time and improves accuracy with limited data

### 3. Backpropagation & optimizer choices

- **Baseline run optimizer (SGD)**: The baseline training run `runs/efficientnet_b3_baseline-20251030-102332` used SGD with momentum (Nesterov) and weight decay. SGD was chosen in that notebook to match the established training regime and to avoid DirectML CPU-fallbacks that can occur with some AdamW operators.

- **Other runs (AdamW)**: Some optimized training notebooks (for example, `train_efficientnet_b3_optimized_food_not_food.ipynb`) use AdamW for weight decay-aware adaptive updates. AdamW is available in the repo and used for experiments, but on DirectML it can trigger CPU fallbacks for certain ops (see note below).

- **Why both exist**: AdamW can offer faster convergence on some tasks; SGD (with momentum) is often more stable and avoids DirectML-related operator fallbacks on AMD/Intel GPUs.

Note: If you run training on DirectML and see warnings about aten::lerp or other ops falling back to CPU when using AdamW, prefer SGD for full-GPU training throughput.

### 4. **Cross-Entropy Loss Function**

- **Formula**: `Loss = -Œ£(y_true √ó log(y_pred))`
- **Purpose**: Measure difference between predicted and actual class
- **Why**: Standard for multi-class classification problems

### 5. **Softmax Activation**

- **Formula**: `softmax(x_i) = e^(x_i) / Œ£(e^(x_j))`
- **Purpose**: Convert raw scores to probabilities (sum to 100%)
- **Output**: Confidence scores for each of 124 classes

### 6. **Cosine Annealing Learning Rate Scheduler**

- **Purpose**: Gradually reduce learning rate following cosine curve
- **Benefit**: Smooth convergence, avoids sharp changes in learning

---

## üíæ Model Deployment

### Export Format: ONNX (Open Neural Network Exchange)

**Specifications**:

- **Input Shape**: `[batch_size, 3, 252, 252]`
- **Output Shape**: `[batch_size, 124]` (probability for each class)
- **File Size**: ~45MB
- **Runtime**: Compatible with ONNX Runtime (CPU/GPU)

**Inference Pipeline**:

```
1. Load image ‚Üí Resize to 252√ó252
2. Normalize with ImageNet statistics
3. Run ONNX model inference
4. Apply softmax to get probabilities
5. Return top-5 predictions with confidence scores
```

**Performance**:

- **CPU Inference**: ~50-150ms per image
- **Memory Usage**: ~200-300MB
- **Deployment**: Web servers (Node.js/Express), Mobile apps, Edge devices

---

## üéì Key Learnings & Best Practices

### What Worked Well ‚úÖ

1. **Two-phase training**: Warmup + fine-tuning prevented catastrophic forgetting
2. **Strong augmentation**: Mixup and geometric transforms improved generalization
3. **Early stopping**: Prevented overfitting and saved computation
4. **High-quality dataset**: Balanced, diverse images led to robust model

### Challenges Encountered ‚ö†Ô∏è

1. **Visually similar classes**: Salads, soups, and pasta dishes harder to distinguish
2. **Training time**: Full fine-tuning ~70 minutes/epoch on DirectML
3. **Class imbalance sensitivity**: Some rare foods need more training examples

### Future Improvements üöÄ

1. **Ensemble models**: Combine multiple models for higher accuracy
2. **Data augmentation++**: Use advanced techniques (CutMix, AutoAugment)
3. **Larger models**: Try EfficientNet-B4 or B5 for marginal gains
4. **Active learning**: Collect more images for challenging classes

---

## üìö References & Resources

### Algorithms & Techniques

1. **EfficientNet**: Tan & Le (2019) - "EfficientNet: Rethinking Model Scaling for CNNs"
2. **Mixup**: Zhang et al. (2018) - "mixup: Beyond Empirical Risk Minimization"
3. **Label Smoothing**: Szegedy et al. (2016) - "Rethinking Inception Architecture"
4. **Transfer Learning**: Pan & Yang (2010) - "A Survey on Transfer Learning"

### Frameworks & Tools

- **PyTorch**: Deep learning framework
- **torchvision**: Pre-trained models and image transforms
- **torch-directml**: AMD/Intel GPU acceleration
- **scikit-learn**: Evaluation metrics
- **ONNX**: Model export for deployment

---

## üìÇ Project Files

```
nutrisight_model_training/
‚îú‚îÄ‚îÄ train_efficientnet_b3_optimized.ipynb   # Training notebook
‚îú‚îÄ‚îÄ README.md                                # This documentation
‚îî‚îÄ‚îÄ efficientnet_b3_optimized-20251021-115920/
    ‚îú‚îÄ‚îÄ best_efficientnet_b3.pth            # Best model checkpoint (PyTorch)
    ‚îú‚îÄ‚îÄ model.onnx                          # Deployment model (ONNX)
    ‚îú‚îÄ‚îÄ class_names.json                    # 124 food category names
    ‚îú‚îÄ‚îÄ summary.json                        # Complete training statistics
    ‚îú‚îÄ‚îÄ metrics_epoch.csv                   # Per-epoch metrics
    ‚îú‚îÄ‚îÄ per_class_accuracy.json             # Accuracy for each food class
    ‚îú‚îÄ‚îÄ per_class_metrics.json              # Precision/Recall per class
    ‚îú‚îÄ‚îÄ classification_report.txt           # Detailed classification report
    ‚îî‚îÄ‚îÄ confusion_matrix.png                # Visual confusion matrix
```

---

## üéØ Conclusion

This study successfully developed a **high-accuracy food recognition model** using deep learning and transfer learning techniques. The final model achieves:

- ‚úÖ **86.87% top-1 accuracy** on 124 food categories (Test set)
- ‚úÖ **97.30% top-5 accuracy** (almost always correct in top 5 guesses)
- ‚úÖ **95.14% accuracy** on high-confidence (Test, ‚â•80%) predictions
- ‚úÖ **Production-ready** ONNX model for web/mobile deployment
- ‚úÖ **Robust performance** across validation and test sets (no overfitting)

The model is suitable for deployment in **nutritional tracking applications**, **restaurant menu digitization**, and **food recognition systems**.

### Statistical Significance

With 4,340 test images and 86.87% accuracy, the model correctly classifies **3,770 out of 4,340 images**, demonstrating strong real-world applicability for food recognition tasks.

---

**Model Training Date**: October 2024  
**Training Duration**: 17 epochs (see run logs for wall-clock time)  
**Hardware**: DirectML-compatible GPU  
**Framework**: PyTorch 2.x + torchvision

---

_For questions or additional information, please refer to the training notebook: `train_efficientnet_b3_optimized.ipynb`_
